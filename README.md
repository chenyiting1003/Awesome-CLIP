# Awesome-CLIP

- [Awesome-CLIP](#awesome-clip)
  - [Train](#train)
  - [Improvement \& Innovation](#improvement--innovation)
  - [Data](#data)
  - [Distillation](#distillation)
  - [Loss](#loss)
  - [Zero-Shot \& Few-Shot \& Classification](#zero-shot--few-shot--classification)
  - [Retrieval](#retrieval)
  - [Segmentation](#segmentation)
  - [Captioning](#captioning)
  - [Other](#other)


## Train


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/Sense-GVT/DeCLIP.svg?style=social&label=Star)](https://github.com/Sense-GVT/DeCLIP) <br> **SUPERVISION EXISTS EVERYWHERE: A DATA EFFICIENT CONTRASTIVE LANGUAGE-IMAGE PRE-TRAINING PARADIGM** <br>| æœ¬æ–‡æå‡ºä¸€ç§åˆ›æ–°çš„CLIPè®­ç»ƒæ–¹å¼--Data efficient CLIP (DeCLIP)ï¼Œæ¥è§£å†³CLIPè®­ç»ƒå¯¹æ–‡æœ¬-å›¾åƒpairæ•°æ®é‡çš„éœ€æ±‚.  æ ¸å¿ƒæ€æƒ³å°±æ˜¯å¢åŠ å¯¹å›¾åƒ-æ–‡æœ¬å¯¹çš„supervision(å¢åŠ æ›´å¤šçº¦æŸ)ï¼Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ é€šç”¨çš„è§†è§‰ç‰¹å¾. ä½œè€…å¢åŠ äº†ä»¥ä¸‹ç›‘ç£ï¼š1.æ¯ä¸ªæ¨¡æ€å†…çš„self-supervision;2.è·¨æ¨¡æ€çš„å¤šè§†å›¾supervision(æ•°æ®å¢å¼ºåçš„view);3.æ¥è‡ªå…¶ä»–ç›¸ä¼¼å¯¹çš„æœ€è¿‘é‚»supervision.  å®éªŒè¯æ˜ï¼Œä¸base CLIPç›¸æ¯”ï¼Œæ›´å°‘çš„è®­ç»ƒæ•°æ®å–å¾—äº†æ›´é«˜çš„è¡¨ç°.  <br>ğŸ§Ÿâ€â™‚ï¸:Nearest-Neighbor Supervisionå¤„è®¾è®¡äº†ä¸€ä¸ªFIFOçš„é˜Ÿåˆ—ï¼Œä¸ªäººè§‰å¾—å€Ÿé‰´äº†MoCoçš„æ€æƒ³ï¼Œå¾ˆæœ‰æ„æ€ğŸ‘ |<img src="./images/DeCLIP.png"  width="1280px"/>| [[Github](https://github.com/Sense-GVT/DeCLIP)] <br> [[Paper](https://arxiv.org/pdf/2110.05208)] |
| [![Star](https://img.shields.io/github/stars/microsoft/RegionCLIP.svg?style=social&label=Star)](https://github.com/microsoft/RegionCLIP) <br> **RegionCLIP: Region-based Language-Image Pretraining** <br>|è™½ç„¶CLIPåœ¨zero-shotå’Œtransfer learning settingsè¿™æ ·çš„å›¾ç‰‡åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä¿—ï¼Œä½†å¦‚æœç›´æ¥å°†CLIPåº”ç”¨åˆ°ç›®æ ‡æ£€æµ‹è¿™ç§è¯†åˆ«å›¾åƒåŒºåŸŸçš„ä»»åŠ¡ä¸Šï¼Œè¡¨ç°å¾ˆå·®. åŸå› æ˜¯å­˜åœ¨domain shiftï¼ŒCLIPåŸºäºimage-text pairçš„æ•°æ®é›†è®­ç»ƒï¼Œå»ºç«‹çš„æ˜¯å›¾ç‰‡æ•´ä½“å’Œæ–‡æœ¬æè¿°çš„è”ç³»ï¼Œä¸æ–‡æœ¬æ²¡æœ‰åšå±€éƒ¨ã€ç»†ç²’åº¦çš„å¯¹é½ . ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºregionCLIP ï¼Œå°†å›¾ç‰‡å±€éƒ¨ä¸å¯¹åº”æè¿°å¯¹é½ï¼Œåœ¨ç‰¹å¾ç©ºé—´å®ç°region-text å¯¹é½. å®éªŒè¯æ˜ï¼Œåœ¨ä¸‹æ¸¸æ£€æµ‹ä»»åŠ¡ä¸Šï¼Œä¸CLIPç›¸æ¯”æ€§èƒ½æ˜æ˜¾æå‡.  <br>ğŸ§Ÿâ€â™‚ï¸: pretrainæœŸé—´æ˜¾å¼å¯¹é½å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬æ ‡è®°ï¼Œæœ‰ä¸ªç‚¹ï¼Œæ— éœ€äººå·¥æ ‡æ³¨regionåŒºåŸŸcaptionï¼Œä½¿ç”¨çš„æ˜¯ä¼ªæ ‡ç­¾ï¼Œå…·ä½“åšæ³•ï¼Œå»ºç«‹ä¸€ä¸ªobjectç±»åˆ«å€™é€‰æ± ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„CLIPï¼Œå¯ä»¥å°†regionï¼ˆRPNæå–ï¼‰ä¸object matchèµ·æ¥ï¼Œç„¶ååˆ©ç”¨å›ºå®šçš„promptæ¨¡ç‰ˆç”Ÿæˆcaptionï¼Œæ¯”å¦‚regionæ˜¯ä¸€åªç‹—ï¼Œæè¿°å°±æ˜¯ a photo of dog.  è¿™é‡Œå¦‚æœå€ŸåŠ©LLaVaç­‰æ¨¡å‹ç”Ÿæˆcaptionï¼Œæ•ˆæœä¼šä¸ä¼šæ›´å¥½ï¼Ÿå½“ç„¶pretrainæˆæœ¬ä¹Ÿä¼šæ›´é«˜ï¼Œéœ€è¦balance. | <img src="./images/regionCLIP.png" width="1280px"/> | [[Github](https://github.com/microsoft/RegionCLIP)] <br> [[Paper](https://arxiv.org/pdf/2112.09106)] |
|2023|
| **Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness** <br>| CLIPæ²¡æœ‰åŒºåˆ†åµŒå…¥åœ¨å›¾åƒä¸­çš„æ–‡æœ¬åŒºåŸŸçš„è§†è§‰è¯­ä¹‰å’Œæ„ä¹‰. å½“åµŒå…¥åŒºåŸŸä¸­çš„æ–‡æœ¬ä¸å›¾åƒçš„è§†è§‰å¤–è§‚ä¸åŒ¹é…æ—¶ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸é²æ£’æ€§. æ–‡ç« æå‡ºä¸¤ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥æé«˜CLIPè®­ç»ƒçš„æ•ˆç‡å’Œé²æ£’æ€§ï¼š1. åœ¨ä¿æŒç›¸åŒæ•°é‡çš„ä¼˜åŒ–æ­¥éª¤çš„åŒæ—¶å¢å¼ºè®­ç»ƒæ•°æ®é›†ï¼›2.è¿‡æ»¤æ‰å›¾åƒä¸­åŒ…å«æ–‡æœ¬åŒºåŸŸçš„æ ·æœ¬.  åœ¨ImageNetå’ŒCoCoç­‰æ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œæ–‡ç« æ–¹æ³•æé«˜äº†Clipåœ¨ä¸‹æ¸¸ä»»åŠ¡çš„åˆ†ç±»å’Œæ£€ç´¢å‡†ç¡®ç‡.  |<img src="./images/LessisMore.png"  width="1280px"/>| [[Paper](https://arxiv.org/pdf/2305.05095)] |
| [![Star](https://img.shields.io/github/stars/UCSC-VLAA/CLIPA.svg?style=social&label=Star)](https://github.com/UCSC-VLAA/CLIPA) <br> **CLIPA: An Inverse Scaling Law for CLIP Training** <br>| æ–‡ç« æå‡ºäº†ä¸€ä¸ªä»¤äººæƒŠè®¶çš„å‘ç°ï¼Œå³CLIPè®­ç»ƒå­˜åœ¨inverse scaling lawï¼Œå³ä½¿ç”¨çš„å›¾åƒ/æ–‡æœ¬ç¼–ç å™¨è¶Šå¤§ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒçš„å›¾åƒ/æ–‡æœ¬tokensçš„åºåˆ—é•¿åº¦è¶ŠçŸ­. æ­¤å¤–ï¼Œå‡å°‘å›¾åƒ/æ–‡æœ¬tokensé•¿åº¦çš„ç­–ç•¥ï¼Œåœ¨ç¡®å®šè¿™ç§ç¼©æ”¾å®šå¾‹çš„è´¨é‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨. æ–‡ç« åœ¨æœ‰é™çš„èµ„æºä¸‹æˆåŠŸè®­ç»ƒäº†Clip. |<img src="./images/CLIPA.png"  width="1280px"/>| [[Github](https://github.com/UCSC-VLAA/CLIPA)] <br> [[Paper](https://arxiv.org/pdf/2305.07017)] |
| [![Star](https://img.shields.io/github/stars/UCSC-VLAA/CLIPA.svg?style=social&label=Star)](https://github.com/UCSC-VLAA/CLIPA) <br> **CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget; An Extra $4,000 Unlocks 81.8% Accuracy** <br>| åœ¨CLIPAåŸºç¡€ä¸Šï¼ŒéªŒè¯äº†full resolution çš„tokenå¾®è°ƒæ¨¡å‹æ—¶ï¼Œinverse scaling lawä¹Ÿé€‚ç”¨;åŒæ—¶éªŒè¯å„ç§ä¸åŒè®­ç»ƒå‚æ•°ä¸‹æ¨¡å‹çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¨¡å‹å¤§å°ã€æ•°æ®å’Œtraining schedule. |<img src="./images/CLIPA-v2.png"  width="1280px"/>| [[Github](https://github.com/UCSC-VLAA/CLIPA)] <br> [[Paper](https://arxiv.org/pdf/2306.15658)] |
| [![Star](https://img.shields.io/github/stars/facebookresearch/flip.svg?style=social&label=Star)](https://github.com/facebookresearch/flip) <br> **Scaling Language-Image Pre-training via Masking** <br>| æ–‡ç« æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„CLIPè®­ç»ƒæ–¹æ³•---FLIPï¼ˆFast Language-Image Pre-trainingï¼‰.è¯¥æ–¹æ³•åªéœ€è¦åœ¨è®­ç»ƒæ—¶éšæœºMaskæ‰ä¸€éƒ¨åˆ†å›¾åƒ. å®éªŒè¯æ˜ï¼Œä¸æ ‡å‡†CLIPè¯¦ç»†ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒé€Ÿåº¦å’Œæ¨¡å‹ç²¾åº¦æ–¹é¢éƒ½æœ‰æå‡. æ–‡ç« å—åˆ°MAEçš„å¯å‘. å¼•å…¥maskingï¼Œä½¿æ¨¡å‹åœ¨â€œhow carefully we look at a sample pairâ€ å’Œ â€œhow many sample pairs we can processâ€ä¹‹é—´åštrade-off. å› ä¸ºVit encoderåªç”¨äºvisible patchesï¼Œå½“maskæ‰ä¸€éƒ¨åˆ†å›¾åƒæ—¶ï¼Œå¯ä»¥èŠ‚çº¦ç›¸åº”çš„æ˜¾å­˜ï¼Œè¿™æ ·é™ä½äº†è®¡ç®—é‡ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„batchsizeï¼Œå¯¹contrastive lossæ›´åŠ å‹å¥½.  åŒæ—¶ï¼Œmaskingä½œä¸ºä¸€ç§å½¢å¼çš„å™ªå£°å’Œæ­£åˆ™åŒ–å¯ä»¥æé«˜é²æ£’æ€§.  |<img src="./images/flip.png"  width="1280px"/>| [[Github](https://github.com/facebookresearch/flip)] <br> [[Paper](https://arxiv.org/pdf/2212.00794)] |
| [![Star](https://img.shields.io/github/stars/LijieFan/LaCLIP.svg?style=social&label=Star)](https://github.com/LijieFan/LaCLIP) <br> **Improving CLIP Training with Language Rewrites** <br>| åœ¨CLIPè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªå¯¹å›¾åƒæ•°æ®åšäº†æ•°æ®å¢å¼ºï¼Œè€Œæ–‡æœ¬æ•°æ®ä¿æŒä¸å˜. é’ˆå¯¹æ­¤é—®é¢˜ï¼Œä½œè€…æå‡ºäº†Language augmented CLIP (LaCLIP), åˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œé‡æ–°æè¿°è®­ç»ƒé›†çš„captionsï¼Œå¢åŠ æ–‡æœ¬çš„å¤šæ ·æ€§. å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸‹æ¸¸zero-shotä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æœ‰æ˜æ˜¾æå‡.|<img src="./images/LaCLIP.png"  width="1280px"/>| [[Github](https://github.com/LijieFan/LaCLIP)] <br> [[Paper](https://arxiv.org/pdf/2305.20088)] |
| [![Star](https://img.shields.io/github/stars/zjukg/Structure-CLIP.svg?style=social&label=Star)](https://github.com/zjukg/Structure-CLIP) <br> **Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations** <br>|CLIPåœ¨ç»“æ„åŒ–çš„æ–‡æœ¬-å›¾åƒåŒ¹é…ä¸Šè¡¨ç°ä¸å¤Ÿï¼Œå¦‚é€šè¿‡clip scoreå¹¶ä¸èƒ½åŒºåˆ«ä¸€å¼ å›¾æ˜¯äººå’¬ç‹—å’Œç‹—å’¬äºº. ä½œè€…è®¤ä¸ºé€ æˆè¿™ä¸ªé—®é¢˜çš„åŸå› æ˜¯CLIPåœ¨å­¦ä¹ å¤šæ¨¡æ€åœºæ™¯ä¸­çš„representationsæ—¶æœªèƒ½å……åˆ†åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†.  æ–‡ç« æå‡º Structure-CLIP ï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé€šè¿‡é›†æˆåœºæ™¯å›¾çŸ¥è¯†æ¥å¢å¼ºå¤šæ¨¡æ€ç»“æ„åŒ–è¡¨ç¤º. <br>ğŸ§Ÿâ€â™‚ï¸: 1.å¢åŠ éš¾ä¾‹è´Ÿæ ·æœ¬; 2. å®é™…å·¥ä½œä¸­ä¸ªäººä¹Ÿæƒ³è¿‡ç±»ä¼¼æ–¹æ³•ï¼Œå¤§æ¦‚æ˜¯é€šè¿‡åˆ†æè´Ÿè´£æè¿°çš„è¯æ€§ç­‰ï¼Œåˆ©ç”¨æœ€çŸ­ä¾èµ–è·¯å¾„ç­‰æ–¹æ³•æ‹†åˆ†å¥å­ï¼Œç„¶ååšenhance.| <img src="./images/Structure-CLIP.png"  width="1280px"/> | [[Github](https://github.com/zjukg/Structure-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2305.06152)] |
| [![Star](https://img.shields.io/github/stars/mertyg/vision-language-models-are-bows.svg?style=social&label=Star)](https://github.com/mertyg/vision-language-models-are-bows) <br> **WHEN AND WHY VISION-LANGUAGE MODELS BEHAVE LIKE BAGS-OF-WORDS, AND WHAT TO DO ABOUT IT?** <br>|  å°½ç®¡VLMåœ¨ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ä¸é”™ï¼Œä½†å¯¹æ¨¡å‹å¦‚ä½•å­¦ä¹ ç±»åˆ«å’Œå±æ€§çš„ç»„åˆå…³ç³»ï¼Œç›®å‰å°šä¸æ¸…æ¥š.  æ–‡ç« é¦–å…ˆæå‡ºä¸€ä¸ªAttribution, Relation, and Order(ARO) çš„benchmarkï¼Œç³»ç»Ÿè¯„ä¼°VLMç†è§£ä¸åŒç±»å‹çš„å…³ç³»ã€å±æ€§å’Œé¡ºåºä¿¡æ¯çš„èƒ½åŠ›. ç„¶åå¯¹ä½¿ç”¨VLMå®ç°çš„æ£€ç´¢ä»»åŠ¡å’Œå¯¹æ¯”é¢„è®­ç»ƒåšäº†æ·±åˆ»è§£æï¼Œæå‡ºè§£é‡Šäº†å‡ ä¸ªé—®é¢˜. æœ€åæå‡ºNegCLIPæé«˜æ¨¡å‹å¯¹ attributes and relationsçš„ç†è§£.  <br>ğŸ§Ÿâ€â™‚ï¸: ICLR 2023 Oralï¼Œåˆçœ‹è§‰å¾—æ²¡ä»€ä¹ˆæ„æ€ï¼Œä½†ç²¾è¯»åæ”¶è·å¾ˆå¤š. æ¯”å¦‚å®é™…å·¥ä½œä¸­ï¼Œåˆ©ç”¨clipå®ç°å¤šæ¨¡æ€æ£€ç´¢ï¼Œè™½ç„¶clipä¸èƒ½å¾ˆå¥½çš„å­¦ä¹ objectå’Œå¯¹åº”å±æ€§çš„å…³ç³»ï¼Œä½†é€šè¿‡è§¦å‘key wordsï¼Œä¾æ—§å¯ä»¥è·å¾—å¾ˆå¥½çš„è¡¨ç°. æ–‡ç« å¯¹æ­¤åšäº†è§£è¯». ä»¥åŠNegCLIPï¼Œæ–¹æ³•ç›´æ¥ç®€å•ï¼Œå…¶å®å®é™…å·¥ä½œä¸­å¸¸ç”¨ç±»ä¼¼æ–¹æ³•. æ€»ä¹‹æ˜¯ä¸€ç¯‡å€¼å¾—ç»†å“çš„æ–‡ç« ğŸ‘| <img src=â€œ./images/NegCLIP.png"  width="1280px"/> | [[Github](https://github.com/mertyg/vision-language-models-are-bows)] <br> [[Paper](https://openreview.net/pdf?id=KRLUvxh8uaX)] |
|2024|
| [![Star](https://img.shields.io/github/stars/YichaoCai1/CLAP.svg?style=social&label=Star)](https://github.com/YichaoCai1/CLAP) <br> **CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts** <br>|ç›´æ¥ä½¿ç”¨[ä½œè€…çš„å›ç­”](https://www.zhihu.com/question/660698707/answer/3550999896)ï¼š ä»causalityç†è®ºå‡ºå‘ï¼ŒCLAPæ—¨åœ¨æå‡pretrained VLMåœ¨distribution shiftåœºæ™¯ä¸‹çš„generalizationèƒ½åŠ›ï¼ŒCLAPä»…éœ€åœ¨text modalityç”¨è¾ƒå°æˆæœ¬å»fine-tune CLIPæ¨¡å‹ï¼Œå¯ä»¥ä½¿pretrained representationsæ›´èšç„¦äºcontentï¼ˆobjectï¼‰æœ¬èº«ï¼Œä»è€Œæå‡æ¨¡å‹zero-shot/few-shotè¡¨ç°, ä»¥åŠdomain adaptationå’Œadversarial resilienceçš„èƒ½åŠ›.  |<img src="./images/CLAP.png"  width="1280px"/>| [[Github](https://github.com/YichaoCai1/CLAP)] <br> [[Paper](https://arxiv.org/pdf/2311.16445)] |
| [![Star](https://img.shields.io/github/stars/apple/ml-tic-clip.svg?style=social&label=Star)](https://github.com/apple/ml-tic-clip) <br> **TIC-CLIP: CONTINUAL TRAINING OF CLIP MODELS** <br> |éšç€æ•°æ®çš„ä¸æ–­ç§¯ç´¯å’Œæ›´æ–°ï¼Œå¦‚ä½•ä½æˆæœ¬çš„è®­ç»ƒæ¨¡å‹ï¼Œä¸æœ€æ–°çš„æ•°æ®åŒæ­¥. æ–‡ç« æå‡ºä¸€ç§ç®€å•çš„rehearsal-basedçš„æ–¹æ¡ˆï¼Œä¸æ ‡å‡†çš„é¢„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œæé€Ÿ2.5x. | <img src="./images/TIC-CLIP.png"  width="1280px"/>| [[Github](https://github.com/apple/ml-tic-clip)] <br> [[Paper](https://arxiv.org/pdf/2310.16226)] |
| [![Star](https://img.shields.io/github/stars/facebookresearch/MetaCLIP.svg?style=social&label=Star)](https://github.com/facebookresearch/MetaCLIP) <br> **DEMYSTIFYING CLIP DATA[MetaCLIP]** <br>|æ–‡ç« æ­ç¤ºäº†CLIPè®­ç»ƒæ•°æ®çš„ç®¡ç†æ–¹æ³•ï¼Œæå‡ºCLIPæ•°æ®ç®¡ç†ç®—æ³•ï¼Œæ¥ç®€åŒ–å’Œäº§ç”Ÿé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®.  å¹¶ä¸”ä»‹ç»äº†Metadata-Curated Language-Image Pre-training (MetaCLIP)---CLIP proç‰ˆ.  <br>ğŸ§Ÿâ€â™‚ï¸:å»ºè®®å¤§è§„æ¨¡æ•°æ®é›†ç®¡ç†é£Ÿç”¨ï¼Œæ•°æ®è´¨é‡æ¯”æ•°é‡é‡è¦å¾—å¤šï½| | [[Github](https://github.com/facebookresearch/MetaCLIP)] <br> [[Paper](https://arxiv.org/pdf/2309.16671)]|


## Improvement & Innovation


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/FlagAI-Open/FlagAI.svg?style=social&label=Star)](https://github.com/FlagAI-Open/FlagAI) <br> **AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities** <br>|æ–‡ç« æå‡ºä¸€ä¸ªæ¦‚å¿µä¸Šç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥è®­ç»ƒä¸€ä¸ªå¼ºå¤§çš„åŒè¯­/å¤šè¯­å¤šæ¨¡æ€è¡¨ç¤ºæ¨¡å‹. ä½¿ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ–‡æœ¬ç¼–ç å™¨XLMRæ›¿æ¢Clipçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒæ¨¡å¼(Teacher Learning; Contrastive Learning)å¯¹é½æ–‡æœ¬å’Œå›¾åƒè¡¨å¾. | <img src="./images/AltCLIP.png"  width="1280px"/> | [[Github](https://github.com/FlagAI-Open/FlagAI)] <br> [[Paper](https://arxiv.org/pdf/2211.06679)] |
|2023|
| [![Star](https://img.shields.io/github/stars/google-research/big_vision.svg?style=social&label=Star)](https://github.com/google-research/big_vision) <br> **CLIPPO: Image-and-Language Understanding from Pixels Only** <br>| æ–‡ç« å¯¹ä½¿ç”¨çº¯åŸºäºåƒç´ çš„æ¨¡å‹è¿›è¡Œæ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€å­¦ä¹ è¿›è¡Œæ¢ç´¢ã€‚CLIPPOæ˜¯ä¸€ä¸ªå•ç‹¬çš„è§†è§‰ Transformerï¼Œå®ƒå¤„ç†è§†è§‰è¾“å…¥æˆ–æ–‡æœ¬ï¼Œæˆ–ä¸¤è€…ä¸€èµ·ï¼Œæ‰€æœ‰éƒ½å‘ˆç°ä¸º RGB å›¾åƒï¼ˆæ–‡æœ¬åœ¨ç©ºç™½å›¾åƒä¸Šæ¸²æŸ“ï¼Œä½œä¸ºçº¯å›¾åƒå¤„ç†ï¼‰. æ‰€æœ‰æ¨¡æ€éƒ½ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å‚æ•°ï¼ŒåŒ…æ‹¬ä½çº§ç‰¹å¾å¤„ç†ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸å­˜åœ¨ç‰¹å®šäºæ¨¡æ€çš„åˆå§‹å·ç§¯ã€tokenization ç®—æ³•æˆ–è¾“å…¥åµŒå…¥è¡¨. CLIPPOä»…ç”¨ä¸€ä¸ªä»»åŠ¡è®­ç»ƒ--å¯¹æ¯”å­¦ä¹ . | <img src="./images/CLIPPO.png"  width="1280px"/> | [[Github](https://github.com/google-research/big_vision)] <br> [[Paper](https://arxiv.org/pdf/2212.08045)] |
| [![Star](https://img.shields.io/github/stars/SunzeY/AlphaCLIP.svg?style=social&label=Star)](https://github.com/SunzeY/AlphaCLIP) <br> **Alpha-CLIP: A CLIP Model Focusing on Wherever You Want** <br>|Clipæ— æ³•å…³æ³¨åˆ°å±€éƒ¨åŒºåŸŸï¼Œé’ˆå¯¹æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºä¸€ä¸ªå¢å¼ºç‰ˆæœ¬çš„CLIPï¼Œåä¸ºAlpha-CLIP. Alpha-CLIPå¸¦æœ‰ä¸€ä¸ªè¾…åŠ©alphaé€šé“æ¥æç¤ºæ³¨æ„åŒºåŸŸï¼Œå¹¶é€šè¿‡æ„é€ æ•°ç™¾ä¸‡ä¸ªRGBAåŒºåŸŸ-æ–‡æœ¬å¯¹è¿›è¡Œäº†å¾®è°ƒã€‚Alpha-CLIPä¸ä»…ä¿ç•™äº†CLIPçš„è§†è§‰è¯†åˆ«èƒ½åŠ›ï¼Œè€Œä¸”å¯ä»¥ç²¾ç¡®æ§åˆ¶å›¾åƒå†…å®¹çš„é‡ç‚¹. å®ƒè¯æ˜äº†åœ¨å„ç§ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¼€æ”¾ä¸–ç•Œè¯†åˆ«ã€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œæ¡ä»¶2D /3Dç”Ÿæˆ. å®ƒå…·æœ‰å¼ºå¤§çš„æ½œåŠ›ï¼Œå¯ä½œä¸ºå›¾åƒç›¸å…³ä»»åŠ¡çš„é€šç”¨å·¥å…·. | <img src="./images/Alpha-CLIP.png"  width="1280px"/> | [[Github](https://github.com/SunzeY/AlphaCLIP)] <br> [[Paper](https://arxiv.org/pdf/2312.03818)] |
| [![Star](https://img.shields.io/github/stars/OFA-Sys/Chinese-CLIP.svg?style=social&label=Star)](https://github.com/OFA-Sys/Chinese-CLIP) <br> **Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese** <br>|æ–‡ç« æå‡ºä¸­æ–‡CLIPé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ç»™å‡ºäº†ä¸¤é˜¶æ®µé¢„è®­ç»ƒæ³•: 1.å°†é¢„è®­ç»ƒClipçš„å›¾åƒç¼–ç å™¨å›ºå®šï¼Œä½¿ç”¨ä¸­æ–‡RoBERTAæ›¿æ¢æ–‡æœ¬ç¼–ç å™¨ï¼Œè®­ç»ƒRoBERTA; 2.æ–‡æœ¬ã€å›¾åƒç¼–ç å™¨åŒæ—¶è®­ç»ƒ.| <img src="./images/Chinese-CLIP.png"  width="1280px"/> | [[Github](https://github.com/OFA-Sys/Chinese-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2211.01335)] |
| [![Star](https://img.shields.io/github/stars/baaivision/EVA.svg?style=social&label=Star)](https://github.com/baaivision/EVA) <br> **EVA-CLIP: Improved Training Techniques for CLIP at Scale** <br>|å¤§åŠ›å‡ºå¥‡è¿¹. | <img src="./images/EVA-CLIP.png"  width="1280px"/> | [[Github](https://github.com/baaivision/EVA)] <br> [[Paper](https://arxiv.org/pdf/2303.15389)] |
|2024|
| [![Star](https://img.shields.io/github/stars/baaivision/EVA.svg?style=social&label=Star)](https://github.com/baaivision/EVA) <br> **EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters** <br>|å¤§åŠ›å‡ºå¥‡è¿¹. | <img src="./images/EVA-CLIP-18B.png"  width="1280px"/> | [[Github](https://github.com/baaivision/EVA)] <br> [[Paper](https://arxiv.org/pdf/2402.04252)] |
| [![Star](https://img.shields.io/github/stars/xmed-lab/CLIP_Surgery.svg?style=social&label=Star)](https://github.com/xmed-lab/CLIP_Surgery) <br> **ACloser Look at the Explainability of Contrastive Language-Image Pre-training** <br>|æ–‡ç« å‘ç°äº†CLIPçš„å¯è§£é‡Šæ€§æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š1.å¯è§†åŒ–ç»“æœå’Œäººçš„æ„ŸçŸ¥æ˜¯åçš„ï¼›2.å¯è§†åŒ–æœ‰éå¸¸å¤šçš„å™ªå£°å“åº”. é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæ–‡ç« é˜è¿°äº†åŸå› ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªtrain-freeçš„è§£å†³æ–¹æ³•. <br>ğŸ§Ÿâ€â™‚ï¸:å·¥ä½œä¸Šé‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œä½¿ç”¨clipåšç›¸ä¼¼å¯¹å¯¹æ¯”ï¼Œcosç›¸ä¼¼åº¦åŸºæœ¬éƒ½åœ¨0.2+ï¼Œè¿™ç¯‡è®ºæ–‡ç»™äº†ç­”æ¡ˆï¼ŒåŒæ—¶Camå›¾çš„ç»“æœæå‡ä¹Ÿå¾ˆå¤§.ğŸ‘ | <img src="./images/CLIP_Surgery.png"  width="1280px"/> | [[Github](https://github.com/xmed-lab/CLIP_Surgery)] <br> [[Paper](https://arxiv.org/pdf/2304.05653)]  [[çŸ¥ä¹](https://www.zhihu.com/question/595372017/answer/2982207851)]|
| [![Star](https://img.shields.io/github/stars/beichenzbc/Long-CLIP.svg?style=social&label=Star)](https://github.com/beichenzbc/Long-CLIP) <br> **Long-CLIP: Unlocking the Long-Text Capability of CLIP** <br>| CLIPçš„æ–‡æœ¬tokené•¿åº¦è¢«é™åˆ¶ä¸º77ï¼Œè€Œç ”ç©¶è¡¨æ˜å®é™…æœ‰æ•ˆé•¿åº¦ç”šè‡³ä¸åˆ°20. è¿™ä½¿å¾—CLIPæ— æ³•å¤„ç†è¯¦ç»†çš„æè¿°,é™åˆ¶äº†å…¶åœ¨å›¾åƒæ£€ç´¢å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢çš„åº”ç”¨. æœ¬æ–‡æå‡ºLong-CLIPä½œä¸ºCLIPçš„å³æ’å³ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒæ”¯æŒé•¿æ–‡æœ¬è¾“å…¥ï¼Œä¿ç•™ç”šè‡³è¶…è¶Šå…¶zero-shotçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è°ƒæ•´CLIPæ½œåœ¨ç©ºé—´ï¼Œä½¿å…¶æ˜“äºå–ä»£CLIPï¼Œè€Œæ— éœ€åœ¨ä¸‹æ¸¸æ¡†æ¶ä¸­è¿›è¡Œä»»ä½•è¿›ä¸€æ­¥çš„è°ƒæ•´.| <img src="./images/Long-CLIP.png"  width="1280px"/> | [[Github](https://github.com/beichenzbc/Long-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2403.15378)]|
| [![Star](https://img.shields.io/github/stars/apple/ml-mobileclip.svg?style=social&label=Star)](https://github.com/apple/ml-mobileclip) <br> **MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training** <br>| æ–‡ç« æå‡ºäº†mobileç‰ˆçš„CLIPï¼Œä¸æ ‡å‡†çš„ViT-B/16 CLIPç›¸æ¯”ï¼Œé€Ÿåº¦æå‡2.3å€ï¼Œåœ¨38ä¸ªæµ‹è¯•é›†ä¸Šaccuracyå¹³å‡æé«˜2.9%. ä¸æ ‡å‡†CLIPç›¸æ¯”ï¼Œè®­ç»ƒæ•ˆç‡æå‡10-1000å€. ä¸»è¦çš„ä¸€äº›ç‚¹åŒ…æ‹¬: æ–‡æœ¬/å›¾åƒencoderçš„é‡æ–°é€‰æ‹©å’Œè®¾è®¡ã€å€ŸåŠ©CoCaå¯¹è®­ç»ƒé›†ç”Ÿæˆå¤šä¸ªcaptionè¿›è¡Œæ•°æ®é›†å¢å¼ºã€å¤šä¸ªå¤§æ¨¡å‹ï¼ˆCLIPï¼‰çš„Model ensemblingï¼Œä»¥åŠåŸºäºæ­¤è®¾è®¡çš„loss.| <img src="./images/MobileCLIP.png"  width="1280px"/> | [[Github](https://github.com/apple/ml-mobileclip)] <br> [[Paper](https://arxiv.org/pdf/2311.17049)]|



## Data


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2024|
| [![Star](https://img.shields.io/github/stars/apple/ml-veclip.svg?style=social&label=Star)](https://github.com/apple/ml-veclip) <br> **VeCLIP: Improving CLIP Training via Visual-enriched Captions** <br>| é’ˆå¯¹ç½‘ç»œçˆ¬è™«çš„æ–‡æœ¬-å›¾åƒæ•°æ®å¯¹ï¼Œè¿›è¡Œcaptioné‡å†™ã€‚ä½¿ç”¨LLaVAç”Ÿæˆcaptionï¼Œç„¶åä¸çˆ¬è™«å¾—åˆ°çš„æè¿°ï¼ˆAltTextsï¼‰åšèåˆï¼Œé€å…¥Vicuna-1.1å¾—åˆ°é‡å†™åçš„caption.  |<img src="./images/VeCLIP.png"  width="1280px"/>|  [[Github](https://github.com/apple/ml-veclip)] <br> [[Paper](https://arxiv.org/pdf/2310.07699)] |
| [![Star](https://img.shields.io/github/stars/hammoudhasan/SynthCLIP.svg?style=social&label=Star)](https://github.com/hammoudhasan/SynthCLIP) <br> **SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?** <br>| ä½¿ç”¨å…¨åˆæˆæ–‡æœ¬å›¾åƒå¯¹è®­ç»ƒ CLIP æ¨¡å‹ï¼Œä¸å…ˆå‰ä¾èµ–äºçœŸå®æ•°æ®çš„æ–¹æ³•æœ‰æ˜¾è‘—åŒºåˆ«ï¼ŒSynthCLIP å®ç°äº†ä¸åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„ CLIP æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½.  |<img src="./images/SynthCLIP.png"  width="1280px"/>|  [[Github](https://github.com/hammoudhasan/SynthCLIP)] <br> [[Paper](https://arxiv.org/pdf/2402.01832)] |



## Distillation


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2023|
| [![Star](https://img.shields.io/github/stars/microsoft/Cream.svg?style=social&label=Star)](https://github.com/microsoft/Cream/tree/main/TinyCLIP) <br> **TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance** <br>|æ–‡ç« æå‡ºäº†ä¸€ç§é¢å‘å¤§è§„æ¨¡è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹çš„è·¨æ¨¡æ€è’¸é¦æ–¹æ³•:TinyCLIP. TinyClipåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯: affinity mimicking and weight inheritance. åŸºäºå¤šçº§æ¸è¿›å¼æ–¹æ¡ˆè¿›è¡Œaffinity mimickingå’ŒWeight inheritanceï¼Œå®ŒæˆClipæ¨¡å‹çš„å‹ç¼©åŠæ€§èƒ½ä¿çœŸï¼Œåœ¨é€Ÿåº¦å’Œå‡†ç¡®åº¦ä¸Šåšäº†è¾ƒå¥½çš„å¹³è¡¡. | <img src="./images/TinyCLIP.png"  width="1280px"/> | [[Github](https://github.com/microsoft/Cream/tree/main/TinyCLIP)] <br> [[Paper](https://arxiv.org/pdf/2211.01335)] |
|2024|
| [![Star](https://img.shields.io/github/stars/winycg/CLIP-KD.svg?style=social&label=Star)](https://github.com/winycg/CLIP-KD) <br> **CLIP-KD: An Empirical Study of CLIP Model Distillation** <br>|æ–‡ç« æ ¸å¿ƒç›®çš„æ˜¯åˆ©ç”¨ä¸€ä¸ªå¤§å‹çš„æ•™å¸ˆCLIPæ¨¡å‹æ¥ç›‘ç£ä¸€ä¸ªå°å‹çš„å­¦ç”ŸCLIPæ¨¡å‹ï¼Œä½¿å¾—å­¦ç”ŸCLIPæ¨¡å‹å¯ä»¥åœ¨ä¿æŒè½»é‡çš„å‰æä¸‹æ˜¾è‘—æå‡æ€§èƒ½. æ–‡ç« ä»å…³ç³»ã€ç‰¹å¾ã€æ¢¯åº¦å’Œå¯¹æ¯”æ¨¡å¼çš„è§’åº¦æ¥æ£€éªŒCLIP-çŸ¥è¯†è’¸é¦çš„æœ‰æ•ˆæ€§ . æœ€åçš„æ¶ˆèå®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç®€å•çš„MSEè¿›è¡Œç‰¹å¾è’¸é¦å®ç°äº†æœ€å¥½çš„è’¸é¦æ€§èƒ½. | <img src="./images/CLIP-KD.png"  width="1280px"/> | [[Github](https://github.com/winycg/CLIP-KD)] <br> [[Paper](https://arxiv.org/pdf/2307.12732)] |



## Loss


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/goel-shashank/CyCLIP.svg?style=social&label=Star)](https://github.com/goel-shashank/CyCLIP) <br> **CYCLIP: Cyclic Contrastive Language-Image Pretraining** <br>|Clipçš„ç›®æ ‡å‡½æ•°ä»…ä½¿ç”¨äº†è·¨æ¨¡æ€çš„å¯¹æ¯”lossï¼Œå¯¹äºå•ä¸ªæ¨¡æ€å†…éƒ¨å’Œè·¨æ¨¡æ€çš„i2tã€t2içš„å¯¹ç§°æ€§çº¦æŸç¨æ˜¾ä¸è¶³ï¼Œå¯èƒ½ä¼šå¯¼è‡´å›¾åƒå’Œæ–‡æœ¬ä¹‹å‰çš„inconsistent predictions. å¦‚æœå¯¹ç§°åŒ–ä¸¤ä¸ªä¸åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ä»¥åŠå›¾åƒ-å›¾åƒå¯¹å’Œæ–‡æœ¬-æ–‡æœ¬å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œåˆ™å¯ä»¥æ¶ˆé™¤å›¾åƒå’Œæ–‡æœ¬ç©ºé—´ä¸­çš„ä¸ä¸€è‡´ï¼ˆçœ‹å›¾ç‰‡æ›´å¥½ç†è§£ï¼‰. è®ºæ–‡æå‡ºäº†cross-modal consistencyå’Œin-modal consistencyä¸¤ç§lossï¼Œä¸æ ‡å‡†clipç›¸æ¯”ï¼Œåœ¨ä¸‹æ¸¸çš„zero-shotåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡æœ‰10% âˆ’ 24%çš„æå‡. | <img src="./images/AltCLIP.png"  width="1280px"/> | [[Github](https://github.com/goel-shashank/CyCLIP)] <br> [[Paper](https://arxiv.org/pdf/2205.14459)] |
|2023|
| [![Star](https://img.shields.io/github/stars/google-research/big_vision.svg?style=social&label=Star)](https://github.com/google-research/big_vision) <br> **SigLip:Sigmoid Loss for Language Image Pre-Training** <br>|æ–‡ç« æå‡ºäº†ä¸€ç§ç”¨äºè¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆSigLIPï¼‰çš„ç®€å•æˆå¯¹ Sigmoid æŸå¤±. ä¸ä½¿ç”¨ Softmax å½’ä¸€åŒ–çš„æ ‡å‡†å¯¹æ¯”å­¦ä¹ ä¸åŒï¼ŒSigmoid æŸå¤±ä»…å¯¹å›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œæ“ä½œï¼Œå¹¶ä¸”ä¸éœ€è¦å¯¹å½’ä¸€åŒ–çš„æˆå¯¹ç›¸ä¼¼æ€§è¿›è¡Œå…¨å±€è§†å›¾.  Sigmoid æŸå¤±åŒæ—¶å…è®¸è¿›ä¸€æ­¥æ‰©å¤§æ‰¹é‡å¤§å°ï¼ŒåŒæ—¶åœ¨è¾ƒå°çš„æ‰¹é‡å¤§å°ä¸‹ä¹Ÿèƒ½è¡¨ç°æ›´å¥½. | <img src="./images/SigLip.png"  width="1280px"/> | [[Github](https://github.com/google-research/big_vision)] <br> [[Paper](https://arxiv.org/pdf/2303.15343)] |



## Zero-Shot & Few-Shot & Classification


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2021|
| [![Star](https://img.shields.io/github/stars/gaopengcuhk/CLIP-Adapter.svg?style=social&label=Star)](https://github.com/gaopengcuhk/CLIP-Adapter) <br> **CLIP-Adapter: Better Vision-Language Models with Feature Adapters** <br>| CLIP Adapteræ˜¯ä¸€ä¸ªä¸ºfew-shot classficationä»»åŠ¡è®¾è®¡çš„ä¸€ä¸ªæ’å…¥å¼æ¨¡å—.åœ¨å†»ä½çš„clipç‰¹å¾ä¸Šæ·»åŠ ä¸€ä¸ªæ®‹å·®è¿æ¥çš„å¾®è°ƒå™¨ï¼Œä½¿å¾—CLIPèƒ½æ›´å¥½åœ°åº”å¯¹åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡.|<img src="./images/CLIP-Adapter.jpg"  width="1280px"/>| [[Github](https://github.com/gaopengcuhk/CLIP-Adapter)] <br> [[Paper](https://arxiv.org/pdf/2110.04544)] |
|2022|
| [![Star](https://img.shields.io/github/stars/gaopengcuhk/Tip-Adapter.svg?style=social&label=Star)](https://github.com/gaopengcuhk/Tip-Adapter) <br> **Tip-Adapter: Training-free Adaption of CLIP** <br>|ä¸ºäº†æé«˜Clipçš„few-shotèƒ½åŠ›ï¼Œæ–‡ç« æå‡ºä¸€ç§å…è®­ç»ƒçš„æ–¹æ³•ï¼Œã€åä¸ºTip-Adapter. é€šè¿‡ä»å°‘æ ·æœ¬ç›‘ç£ä¸­æ„å»ºquery-keyç¼“å­˜æ¨¡å‹æ¥è·å–é€‚é…å™¨çš„æƒé‡. é€šè¿‡ç¼“å­˜æ¨¡å‹ï¼Œä¸ä¼ ç»Ÿçš„finetuneæ–¹æ³•ç›¸æ¯”ï¼ŒTip-Adapterè¡¨ç°å‡ºæé«˜çš„æ•ˆç‡. |<img src="./images/Tip-Adapter.png"  width="1280px"/>| [[Github](https://github.com/gaopengcuhk/Tip-Adapter)] <br> [[Paper]( https://arxiv.org/pdf/2207.09519)]  |
| [![Star](https://img.shields.io/github/stars/ZiyuGuo99/CALIP.svg?style=social&label=Star)](https://github.com/ZiyuGuo99/CALIP) <br> **CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention** <br>| æ–‡ç« å‡ºå‘ç‚¹æ˜¯å¦‚ä½•åœ¨ä¸finetuneçš„æƒ…å†µä¸‹ï¼Œæå‡clipåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„zero-shotèƒ½åŠ›.æ–‡ç« æå‡ºäº†ä¸€ç§parameter-freeçš„æ³¨æ„åŠ›æ¨¡å—(CALIP)ï¼Œå¼•å¯¼è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºç›¸äº’äº¤äº’ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æ¢ç´¢è·¨æ¨¡å¼ä¿¡æ¯ç‰¹å¾.é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå›¾åƒä¸æ–‡æœ¬ä¸¤ä¸ªæ¨¡æ€ç‰¹å¾ç›¸äº’æ„ŸçŸ¥ï¼Œä»¥å®ç°æ›´å¥½çš„è‡ªé€‚åº”é›¶æ ·æœ¬å¯¹é½.|<img src="./images/CALIP.png"  width="1280px"/>| [[Github](https://github.com/ZiyuGuo99/CALIP)] <br> [[Paper](https://arxiv.org/pdf/2209.14169)]  |
| [![Star](https://img.shields.io/github/stars/KaiyangZhou/CoOp.svg?style=social&label=Star)](https://github.com/KaiyangZhou/CoOp)   <br> **CoOp: Learning to Prompt for Vision-Language Models** <br>| å—NLPé¢†åŸŸprompt learningçš„å¯å‘ï¼Œæ–‡ç« æå‡ºäº†Context Optimization(CoOp)ï¼Œç”¨äºå°†ç±»CLIPå¼çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿ç§»åˆ°ä¸‹æ¸¸å›¾åƒè¯†åˆ«ä»»åŠ¡.å…·ä½“è€Œè¨€ï¼ŒCoOpå°†é¢„è®­ç»ƒæ¨¡å‹å‚æ•°freezeï¼Œä½¿ç”¨å¯å­¦ä¹ å‘é‡å¯¹æç¤ºçš„ä¸Šä¸‹æ–‡å•è¯è¿›è¡Œå»ºæ¨¡.ä½œè€…åœ¨11ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸ŠéªŒè¯CoOpçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCoOpçš„æ€§èƒ½æ˜æ˜¾å¥½äºåŸå§‹é¢„è®­ç»ƒæ¨¡å‹å¦‚CLIP.|<img src="./images/CoOp.png"  width="1280px"/>| [[Github](https://github.com/KaiyangZhou/CoOp)] <br> [[Paper](https://arxiv.org/pdf/2109.01134)] |
| [![Star](https://img.shields.io/github/stars/KaiyangZhou/CoOp.svg?style=social&label=Star)](https://github.com/KaiyangZhou/CoOp)   <br> **CoCoOp: Conditional Prompt Learning for Vision-Language Models** <br>|é’ˆå¯¹CoOpæ³›åŒ–æ€§å·®çš„é—®é¢˜ï¼Œå³:å­¦ä¹ åˆ°çš„ä¸Šä¸‹æ–‡å¯¹æ•°æ®é›†ä¸­unseen classesçš„æ³›åŒ–æ€§ä¸å¥½.æ–‡ç« æå‡ºConditional Context Optimization (CoCoOp)ï¼Œåœ¨CoOpåŸºç¡€ä¸Šï¼Œå¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„ç½‘ç»œï¼Œåä¸ºMeta-Net:ä¸ºæ¯å¼ å›¾åƒç”Ÿæˆinput-conditional tokens. input-conditional tokensä¸ CoOpä¸­çš„learnable vectorså åŠ ï¼Œå…±åŒå‚ä¸è®­ç»ƒ.å¤§é‡å®éªŒè¡¨æ˜ï¼Œå¯¹äºunseen classesï¼ŒCoCoOp æ¯” CoOp çš„æ³›åŒ–èƒ½åŠ›è¦å¥½å¾—å¤šï¼Œç”šè‡³æ˜¾ç¤ºå‡ºè¶…è¶Šå•ä¸ªæ•°æ®é›†çš„å¯è¿ç§»æ€§ï¼Œ å¹¶äº§ç”Ÿæ›´å¼ºçš„é¢†åŸŸæ³›åŒ–æ€§èƒ½ |<img src="./images/CoCoOp.png"  width="1280px"/>| [[Github](https://github.com/KaiyangZhou/CoOp)] <br> [[Paper](https://arxiv.org/pdf/2203.05557)] |
| [![Star](https://img.shields.io/github/stars/LightDXY/FT-CLIP.svg?style=social&label=Star)](https://github.com/LightDXY/FT-CLIP)   <br> **CLIP Itself is a Strong Fine-tuner:Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet** <br>| æ–‡ç« é€šè¿‡ä¸€ç³»åˆ—è¯•éªŒï¼ŒéªŒè¯ä½¿ç”¨ä¸åŒè¶…å‚æ•°finetune clipååœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡çš„è¡¨ç°.  |<img src="./images/FT-CLIP.png"  width="1280px"/>| [[Github](https://github.com/LightDXY/FT-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2212.06138)] |
|[![Star](https://img.shields.io/github/stars/xmed-lab/CLIPN.svg?style=social&label=Star)](https://github.com/xmed-lab/CLIPN)   <br> **CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No** <br>| æ–‡ç« çš„motivationæ˜¯é€šè¿‡å‘CLIPæä¾›positiveçš„è¯­ä¹‰æç¤ºå’Œnegativeçš„è¯­ä¹‰æç¤ºï¼Œä»¥æ­¤è®©CLIPæ‹¥æœ‰åŒºåˆ†OODï¼ˆOut-of-distributionï¼‰å’ŒIDï¼ˆin-distributionï¼‰æ ·æœ¬çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´,æ–‡ç« è®¾è®¡äº†ä¸€ä¸ªå¯å­¦ä¹ çš„"å¦å®š"æç¤ºåŠä¸€ä¸ªé’ˆå¯¹"å¦å®š"çš„æ–‡æœ¬ç¼–ç å™¨,ä»¥æ•æ‰å›¾åƒä¸­çš„å¦å®šè¯­ä¹‰.|<img src="./images/CLIPN.png"  width="1280px"/>| [[Github](https://github.com/xmed-lab/CLIPN)] <br> [[Paper](https://arxiv.org/pdf/2308.12213v2)] |
|[![Star](https://img.shields.io/github/stars/muzairkhattak/multimodal-prompt-learning.svg?style=social&label=Star)](https://github.com/muzairkhattak/multimodal-prompt-learning)   <br> **MaPLe: Multi-modal Prompt Learning** <br>|è¯¸å¦‚CLIPçš„VLMæ¨¡å‹ï¼Œå¯¹æ–‡æœ¬æç¤ºå¾ˆæ•æ„Ÿï¼Œéœ€è¦ä»”ç»†é€‰æ‹©promptæ‰èƒ½å‘æŒ¥è‰¯å¥½ä½œç”¨.  é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼Œè¿‘æœŸæ¯”è¾ƒå¸¸ç”¨å­¦ä¹ æç¤ºä½œä¸ºæ–‡æœ¬è¾“å…¥çš„æ–¹æ¡ˆï¼ˆlearn promptsï¼‰å¾®è°ƒCLIP.  ä½œè€…è®¤ä¸ºåªåœ¨å•ä¸ªbranchä¸­ä½¿ç”¨æç¤ºæ¥è°ƒæ•´ CLIPçš„è¡¨ç¤ºå¹¶ä¸æ˜¯æœ€ä¼˜çš„ï¼Œå› ä¸ºå®ƒä¸å…è®¸çµæ´»åœ°åŠ¨æ€è°ƒæ•´ä¸‹æ¸¸ä»»åŠ¡çš„ä¸¤ä¸ªè¡¨ç¤ºç©ºé—´.  ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†MaPLeï¼Œåœ¨imageå’Œtextä¸¤ä¸ªåˆ†æ”¯éƒ½è¿›è¡Œprompt learningï¼Œæé«˜ä¸¤ä¸ªæ¨¡æ€çš„è¡¨å¾ä¸€è‡´æ€§. ä¸CoCoOp ç›¸æ¯”ï¼ŒMaPLe è¡¨ç°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ï¼Œåœ¨ 11 ä¸ªä¸åŒçš„å›¾åƒè¯†åˆ«æ•°æ®é›†æ€§èƒ½å¹³å‡æ˜æ˜¾æå‡. |<img src="./images/MaPLe.png"  width="1280px"/>| [[Github](https://github.com/muzairkhattak/multimodal-prompt-learning)] <br> [[Paper](https://arxiv.org/pdf/2210.03117)] |
|2024|
| [![Star](https://img.shields.io/github/stars/TJU-sjyj/AMU-Tuning.svg?style=social&label=Star)](https://github.com/TJU-sjyj/AMU-Tuning) <br> **AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning** <br>| æå‡ºäº†ä¸€ç§åä¸ºAMU-Tuningçš„æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›åŸºäºCLIPæ¨¡å‹çš„å°æ ·æœ¬å­¦ä¹ æ€§èƒ½. è¯¥æ–¹æ³•é€šè¿‡åˆ†æå…³é”®ç»„ä»¶â€”â€”logitç‰¹å¾ã€logité¢„æµ‹å™¨å’Œlogitèåˆâ€”â€”æ¥å­¦ä¹ æœ‰æ•ˆçš„logitåå·®ï¼Œå¹¶é€šè¿‡åˆ©ç”¨è¾…åŠ©ç‰¹å¾ã€å¤šåˆ†æ”¯è®­ç»ƒçš„ç‰¹å¾åˆå§‹åŒ–çº¿æ€§åˆ†ç±»å™¨ä»¥åŠåŸºäºä¸ç¡®å®šæ€§çš„èåˆç­–ç•¥ï¼Œå°†logitåå·®æœ‰æ•ˆåœ°æ•´åˆåˆ°CLIPä¸­ï¼Œä»¥æé«˜å°æ ·æœ¬åˆ†ç±»çš„å‡†ç¡®æ€§. |<img src="./images/AMU-Tuning.png"  width="1280px"/>| [[Github](https://github.com/TJU-sjyj/AMU-Tuning)] <br> [[Paper](https://arxiv.org/pdf/2404.089588)] |



## Retrieval


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2023|
| [![Star](https://img.shields.io/github/stars/ABaldrati/CLIP4Cir.svg?style=social&label=Star)](https://github.com/ABaldrati/CLIP4Cir) <br> **Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features** <br>| ä½¿ç”¨clipè¿›è¡Œæ£€ç´¢. åˆ†ä¸ºä¸¤æ­¥ï¼š1.å¾®è°ƒclipçš„text encoder å’Œimage encoderï¼›2.è®¾è®¡ä¸€ä¸ªcombinerï¼Œå°†ä¸¤ä¸ªæ¨¡æ€ç‰¹å¾fusionï¼Œç”¨è¿™ä¸ªç‰¹å¾åšretrieval. |<img src="./images/CLIP4Cir1.png"  width="640px"/>   <img src="./images/CLIP4Cir2.png"  width="640px"/>| [[Github](https://github.com/ABaldrati/CLIP4Cir)] <br> [[Paper](https://arxiv.org/pdf/2308.11485)] |
|2024|
| **JINA CLIP: Your CLIP Model Is Also Your Text Retriever** <br>| ä¼ ç»Ÿçš„text embeddingæ¨¡å‹ï¼Œåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬æ£€ç´¢ä¸­å‡ºè‰²ï¼Œä½†æ— æ³•æ‰§è¡Œcross-modalä»»åŠ¡. è¯¸å¦‚Clipä¹‹ç±»çš„æ¨¡å‹ï¼Œæœ‰æ•ˆåœ°å¯¹é½å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ï¼Œä½†ç”±äºå…¶è®­ç»ƒæ–¹æ³•å’Œä¸Šä¸‹æ–‡é™åˆ¶ï¼Œå› æ­¤æœªé’ˆå¯¹æ–‡æœ¬åˆ°æ–‡æœ¬æ£€ç´¢è¿›è¡Œä¼˜åŒ–. æ–‡ç« æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šä»»åŠ¡å¯¹æ¯”è®­ç»ƒæ–¹æ³•ï¼Œåœ¨å•ä¸ªæ¨¡å‹ä¸­å®ç°äº†state-of-the-artçš„æ–‡æœ¬åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢èƒ½åŠ›. |<img src="./images/JINA-CLIP.png"  width="640px"/>   | [[huggingface](https://huggingface.co/jinaai/jina-clip-v1)] <br> [[Paper](https://arxiv.org/pdf/2405.20204))] |



## Segmentation


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/raoyongming/DenseCLIP.svg?style=social&label=Star)](https://github.com/raoyongming/DenseCLIP) <br> **DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting** <br>| æ–‡ç« æå‡ºä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå°†clipçš„é¢„è®­ç»ƒçŸ¥è¯†è¿ç§»åˆ°ä¸‹æ¸¸åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ç­‰å¯†é›†ä»»åŠ¡. ä½œè€…å°†CLIP ä¸­çš„å›¾åƒ-æ–‡æœ¬åŒ¹é…é—®é¢˜è½¬æ¢ä¸ºåƒç´ æ–‡æœ¬åŒ¹é…é—®é¢˜ï¼Œå¹¶ä½¿ç”¨åƒç´ -æ–‡æœ¬åŒ¹é…é—®é¢˜ï¼Œä½¿ç”¨åƒç´ -æ–‡æœ¬åŒ¹é…å¾—åˆ†(pixel-text score maps)æ¥æŒ‡å¯¼å¯†é›†é¢„æµ‹æ¨¡å‹çš„å­¦ä¹ .  é€šè¿‡è¿›ä¸€æ­¥ä½¿ç”¨å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æç¤ºè¯­è¨€æ¨¡å‹ï¼Œä¿ƒè¿›æ¨¡å‹æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒçš„çŸ¥è¯†. |<img src="./images/DenseCLIP.png"  width="1280px"/>| [[Github](https://github.com/raoyongming/DenseCLIP)] <br> [[Paper](https://arxiv.org/pdf/2112.01518)] |


## Captioning


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2021|
| [![Star](https://img.shields.io/github/stars/rmokady/CLIP_prefix_caption.svg?style=social&label=Star)](https://github.com/rmokady/CLIP_prefix_caption) <br> **ClipCap: CLIP Prefix for Image Captioning** <br>| ä½œè€…æå‡ºäº†CLIPCapæ¨¡å‹æ¥ç”Ÿæˆimage captions.å…·ä½“è€Œè¨€ï¼Œå€ŸåŠ©CLIPæå–å›¾åƒembeaddingï¼Œè®­ç»ƒä¸€ä¸ªmapping networkï¼Œä¸ºæ¯ä¸€ä¸ªcaptionç”Ÿæˆå‰ç¼€.ç›´æ¥å’Œcaption embedding åšç»“åˆï¼ˆconcatenationï¼‰ï¼Œå½¢æˆæ–°çš„embeddingï¼Œé€å…¥GPT-2ç”Ÿæˆcaptions. |<img src="./images/ClipCap.png"  width="1280px"/>| [[Github](https://github.com/rmokady/CLIP_prefix_caption)] <br> [[Paper](https://arxiv.org/pdf/2111.09734)] |
|2022|
| [![Star](https://img.shields.io/github/stars/DavidHuji/CapDec.svg?style=social&label=Star)](https://github.com/DavidHuji/CapDec) <br> **CapDec: Text-Only Training for Image Captioning using Noise-Injected CLIP** <br>| æ–‡ç« è®¤ä¸ºï¼ŒClipæ¨¡å‹çš„è®­ç»ƒï¼Œå°±æ˜¯å°†æŠ½å–çš„æ–‡æœ¬å’Œå›¾ç‰‡ç‰¹å¾å°½å¯èƒ½ç›¸ä¼¼. åŸºäºè¿™ä¸ªè§‚å¯Ÿï¼Œåªéœ€è¦è®¾è®¡ä¸€ä¸ªdecoderï¼Œä»…åˆ©ç”¨æ–‡æœ¬æ•°æ®å­¦ä¹ å¦‚ä½•å°†æ–‡æœ¬ç‰¹å¾â€œç¿»è¯‘â€åˆ°æ–‡æœ¬ï¼Œå³å¯å®ç°å›¾ç‰‡captioning.  <br>ğŸ§Ÿâ€â™‚ï¸:è„‘æ´å¾ˆå¤§ï¼Œåˆå¾ˆåˆç†ï¼Œå–œæ¬¢è¿™ç¯‡æ–‡ç« ~ğŸ‘|<img src="./images/CapDec.png"  width="1280px"/>| [[Github](https://github.com/DavidHuji/CapDec)] <br> [[Paper](https://arxiv.org/pdf/2211.00575)] |
|2023|
| [![Star](https://img.shields.io/github/stars/dhg-wei/DeCap.svg?style=social&label=Star)](https://github.com/dhg-wei/DeCap) <br> **DECAP: DECODING CLIP LATENTS FOR ZERO-SHOT CAPTIONING VIA TEXT-ONLY TRAINING** <br>|æ–‡ç« æå‡ºä¸€ä¸ªç®€å•çš„æ¡†æ¶æ¥å®ç°Zero-shot Captioning. clipçš„ text encoderä½œä¸ºè¾“å…¥ï¼Œä½¿ç”¨text-onlyæ•°æ®è®­ç»ƒä¸€ä¸ªtext decoderã€‚åŒæ—¶ï¼Œä¸ºäº†è§£å†³å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ä¸­çš„modality gapé—®é¢˜ï¼Œä½œè€…å°† image embedding é€å…¥ text decoder ä¸­è§£ç ï¼Œå®ç° Zero-shot Captioning.  |<img src="./images/DeCap.png"  width="1280px"/>| [[Github](https://github.com/dhg-wei/DeCap)] <br> [[Paper](https://openreview.net/pdf?id=Lt8bMlhiwx2)] |


## Other


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/Sense-GVT/DeCLIP.svg?style=social&label=Star)](https://github.com/Sense-GVT/DeCLIP) <br> **Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision** <br>| æ–‡ç« æå‡ºCLIP-benchmarkï¼Œæ˜¯ç¬¬ä¸€ä¸ªå¯¹CLIPåŠå…¶å˜ä½“è¿›è¡Œè¯„ä¼°ã€åˆ†æå’Œæµ‹è¯•çš„åŸºå‡†. åŒæ—¶ï¼Œä½œè€…æå‡ºä¸‰ä¸ªè§‚ç‚¹ï¼Œ1.æ•°æ®è´¨é‡å¯¹æ€§èƒ½æœ‰å¾ˆå¤§å½±å“ï¼›2..æŸäº›supervisionå¯¹å·ç§¯ç½‘ç»œï¼ˆConvNetsï¼‰å’Œè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰æœ‰ä¸åŒçš„å½±å“. é€‚å½“çš„supervisionå¯ä»¥æœ‰æ•ˆåœ°æé«˜CLIPçš„æ€§èƒ½; 3.å‡å°‘æ–‡æœ¬ç¼–ç å™¨å¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼Œä½†å¯¹æœ€ç»ˆæ€§èƒ½å½±å“ä¸å¤§.  æ­¤å¤–ï¼Œä½œè€…å°†DeCLIPä¸FLIPç»“åˆï¼Œå¾—åˆ°ä¸€ä¸ªæ€§èƒ½è¾ƒå¥½çš„CLIPå˜ä½“: DeFILIP.|| [[Github](https://github.com/Sense-GVT/DeCLIP)] <br> [[Paper](https://arxiv.org/pdf/2203.05796)] |
| [![Star](https://img.shields.io/github/stars/IceClear/CLIP-IQA.svg?style=social&label=Star)](https://github.com/IceClear/CLIP-IQA) <br> **Exploring CLIP for Assessing the Look and Feel of Images** <br>| å€ŸåŠ©CLIPåšIQAï¼Œæ–‡ç« å®éªŒè¯æ˜ï¼ŒCLIP æ•è·äº†æœ‰æ„ä¹‰çš„priorsï¼Œå¯ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°ä¸åŒçš„æ„ŸçŸ¥è¯„ä¼° . | <img src="./images/CLIP-IQA.png" width="1280px"/> | [[Github](https://github.com/IceClear/CLIP-IQA)] <br> [[Paper](https://arxiv.org/pdf/2207.12396)] |
|2024|
| [![Star](https://img.shields.io/github/stars/MrChenFeng/CLIPCleaner_ACMMM2024.svg?style=social&label=Star)](https://github.com/MrChenFeng/CLIPCleaner_ACMMM2024) <br> **CLIPCleaner: Cleaning Noisy Labels with CLIP** <br>| æ–‡ç« é¦–æ¬¡æå‡ºåˆ©ç”¨ VLæ¨¡å‹è¿›è¡Œæ ·æœ¬é€‰æ‹©æ¥è§£å†³å™ªå£°æ ‡ç­¾å­¦ä¹  (LNL) é—®é¢˜.  ä½œè€…ä½¿ç”¨CLIPï¼Œå€ŸåŠ©å…¶å¤©ç„¶çš„å¤šæ¨¡æ€å’Œzero-shotèƒ½åŠ›ï¼Œæå‡ºä¸€ç§CLIPCleanerçš„æ–¹æ³•æ¥è¿›è¡Œæ ·æœ¬é€‰æ‹©ï¼Œå¹¶ä»ç†è®ºå’Œç»éªŒä¸Šè¯å®äº†å…¶æœ‰æ•ˆæ€§ï¼ˆå¤§é‡å…¬å¼è­¦å‘Šï¼‰ . | <img src="./images/CLIPCleaner.png" width="1280px"/> | [[Github](https://github.com/MrChenFeng/CLIPCleaner_ACMMM2024)] <br> [[Paper](https://www.arxiv.org/pdf/2408.10012)] |

