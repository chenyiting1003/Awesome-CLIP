# Awesome-CLIP

- [Awesome-CLIP](#awesome-clip)
  - [Train](#train)
  - [Improvement \& Innovation](#improvement--innovation)
  - [Data](#data)
  - [Distillation](#distillation)
  - [Loss](#loss)
  - [Zero-Shot \& Few-Shot \& Classification](#zero-shot--few-shot--classification)
  - [Retrieval](#retrieval)
  - [Detection](#detection)
  - [Segmentation](#segmentation)
  - [Captioning](#captioning)
  - [Generation](#generation)
  - [Video](#video)
  - [Other](#other) 


## Train


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/Sense-GVT/DeCLIP.svg?style=social&label=Star)](https://github.com/Sense-GVT/DeCLIP) <br> **SUPERVISION EXISTS EVERYWHERE: A DATA EFFICIENT CONTRASTIVE LANGUAGE-IMAGE PRE-TRAINING PARADIGM** <br>| æœ¬æ–‡æå‡ºä¸€ç§åˆ›æ–°çš„CLIPè®­ç»ƒæ–¹å¼--Data efficient CLIP (DeCLIP)ï¼Œæ¥è§£å†³CLIPè®­ç»ƒå¯¹æ–‡æœ¬-å›¾åƒpairæ•°æ®é‡çš„éœ€æ±‚.  æ ¸å¿ƒæ€æƒ³å°±æ˜¯å¢åŠ å¯¹å›¾åƒ-æ–‡æœ¬å¯¹çš„supervision(å¢åŠ æ›´å¤šçº¦æŸ)ï¼Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ é€šç”¨çš„è§†è§‰ç‰¹å¾. ä½œè€…å¢åŠ äº†ä»¥ä¸‹ç›‘ç£ï¼š1.æ¯ä¸ªæ¨¡æ€å†…çš„self-supervision;2.è·¨æ¨¡æ€çš„å¤šè§†å›¾supervision(æ•°æ®å¢å¼ºåçš„view);3.æ¥è‡ªå…¶ä»–ç›¸ä¼¼å¯¹çš„æœ€è¿‘é‚»supervision.  å®éªŒè¯æ˜ï¼Œä¸base CLIPç›¸æ¯”ï¼Œæ›´å°‘çš„è®­ç»ƒæ•°æ®å–å¾—äº†æ›´é«˜çš„è¡¨ç°.  <br><br>ğŸ§Ÿâ€â™‚ï¸:Nearest-Neighbor Supervisionå¤„è®¾è®¡äº†ä¸€ä¸ªFIFOçš„é˜Ÿåˆ—ï¼Œä¸ªäººè§‰å¾—å€Ÿé‰´äº†MoCoçš„æ€æƒ³ï¼Œå¾ˆæœ‰æ„æ€ğŸ‘ |<img src="./images/DeCLIP.png"  width="640px"/>| [[Github](https://github.com/Sense-GVT/DeCLIP)] <br> [[Paper](https://arxiv.org/pdf/2110.05208)] |
|2023|
| **Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness** <br>| CLIPæ²¡æœ‰åŒºåˆ†åµŒå…¥åœ¨å›¾åƒä¸­çš„æ–‡æœ¬åŒºåŸŸçš„è§†è§‰è¯­ä¹‰å’Œæ„ä¹‰. å½“åµŒå…¥åŒºåŸŸä¸­çš„æ–‡æœ¬ä¸å›¾åƒçš„è§†è§‰å¤–è§‚ä¸åŒ¹é…æ—¶ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸é²æ£’æ€§. æ–‡ç« æå‡ºä¸¤ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥æé«˜CLIPè®­ç»ƒçš„æ•ˆç‡å’Œé²æ£’æ€§ï¼š1. åœ¨ä¿æŒç›¸åŒæ•°é‡çš„ä¼˜åŒ–æ­¥éª¤çš„åŒæ—¶å¢å¼ºè®­ç»ƒæ•°æ®é›†ï¼›2.è¿‡æ»¤æ‰å›¾åƒä¸­åŒ…å«æ–‡æœ¬åŒºåŸŸçš„æ ·æœ¬.  åœ¨ImageNetå’ŒCoCoç­‰æ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œæ–‡ç« æ–¹æ³•æé«˜äº†Clipåœ¨ä¸‹æ¸¸ä»»åŠ¡çš„åˆ†ç±»å’Œæ£€ç´¢å‡†ç¡®ç‡.  |<img src="./images/LessisMore.png"  width="640px"/>| [[Paper](https://arxiv.org/pdf/2305.05095)] |
| [![Star](https://img.shields.io/github/stars/UCSC-VLAA/CLIPA.svg?style=social&label=Star)](https://github.com/UCSC-VLAA/CLIPA) <br> **CLIPA: An Inverse Scaling Law for CLIP Training** <br>| æ–‡ç« æå‡ºäº†ä¸€ä¸ªä»¤äººæƒŠè®¶çš„å‘ç°ï¼Œå³CLIPè®­ç»ƒå­˜åœ¨inverse scaling lawï¼Œå³ä½¿ç”¨çš„å›¾åƒ/æ–‡æœ¬ç¼–ç å™¨è¶Šå¤§ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒçš„å›¾åƒ/æ–‡æœ¬tokensçš„åºåˆ—é•¿åº¦è¶ŠçŸ­. æ­¤å¤–ï¼Œå‡å°‘å›¾åƒ/æ–‡æœ¬tokensé•¿åº¦çš„ç­–ç•¥ï¼Œåœ¨ç¡®å®šè¿™ç§ç¼©æ”¾å®šå¾‹çš„è´¨é‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨. æ–‡ç« åœ¨æœ‰é™çš„èµ„æºä¸‹æˆåŠŸè®­ç»ƒäº†Clip. |<img src="./images/CLIPA.png"  width="640px"/>| [[Github](https://github.com/UCSC-VLAA/CLIPA)] <br> [[Paper](https://arxiv.org/pdf/2305.07017)] |
| [![Star](https://img.shields.io/github/stars/UCSC-VLAA/CLIPA.svg?style=social&label=Star)](https://github.com/UCSC-VLAA/CLIPA) <br> **CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget; An Extra $4,000 Unlocks 81.8% Accuracy** <br>| åœ¨CLIPAåŸºç¡€ä¸Šï¼ŒéªŒè¯äº†full resolution çš„tokenå¾®è°ƒæ¨¡å‹æ—¶ï¼Œinverse scaling lawä¹Ÿé€‚ç”¨;åŒæ—¶éªŒè¯å„ç§ä¸åŒè®­ç»ƒå‚æ•°ä¸‹æ¨¡å‹çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¨¡å‹å¤§å°ã€æ•°æ®å’Œtraining schedule. |<img src="./images/CLIPA-v2.png"  width="640px"/>| [[Github](https://github.com/UCSC-VLAA/CLIPA)] <br> [[Paper](https://arxiv.org/pdf/2306.15658)] |
| [![Star](https://img.shields.io/github/stars/facebookresearch/flip.svg?style=social&label=Star)](https://github.com/facebookresearch/flip) <br> **Scaling Language-Image Pre-training via Masking** <br>| æ–‡ç« æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„CLIPè®­ç»ƒæ–¹æ³•---FLIPï¼ˆFast Language-Image Pre-trainingï¼‰.è¯¥æ–¹æ³•åªéœ€è¦åœ¨è®­ç»ƒæ—¶éšæœºMaskæ‰ä¸€éƒ¨åˆ†å›¾åƒ. å®éªŒè¯æ˜ï¼Œä¸æ ‡å‡†CLIPè¯¦ç»†ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒé€Ÿåº¦å’Œæ¨¡å‹ç²¾åº¦æ–¹é¢éƒ½æœ‰æå‡. æ–‡ç« å—åˆ°MAEçš„å¯å‘. å¼•å…¥maskingï¼Œä½¿æ¨¡å‹åœ¨â€œhow carefully we look at a sample pairâ€ å’Œ â€œhow many sample pairs we can processâ€ä¹‹é—´åštrade-off. å› ä¸ºVit encoderåªç”¨äºvisible patchesï¼Œå½“maskæ‰ä¸€éƒ¨åˆ†å›¾åƒæ—¶ï¼Œå¯ä»¥èŠ‚çº¦ç›¸åº”çš„æ˜¾å­˜ï¼Œè¿™æ ·é™ä½äº†è®¡ç®—é‡ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„batchsizeï¼Œå¯¹contrastive lossæ›´åŠ å‹å¥½.  åŒæ—¶ï¼Œmaskingä½œä¸ºä¸€ç§å½¢å¼çš„å™ªå£°å’Œæ­£åˆ™åŒ–å¯ä»¥æé«˜é²æ£’æ€§.  |<img src="./images/flip.png"  width="640px"/>| [[Github](https://github.com/facebookresearch/flip)] <br> [[Paper](https://arxiv.org/pdf/2212.00794)] |
| [![Star](https://img.shields.io/github/stars/LijieFan/LaCLIP.svg?style=social&label=Star)](https://github.com/LijieFan/LaCLIP) <br> **Improving CLIP Training with Language Rewrites** <br>| åœ¨CLIPè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªå¯¹å›¾åƒæ•°æ®åšäº†æ•°æ®å¢å¼ºï¼Œè€Œæ–‡æœ¬æ•°æ®ä¿æŒä¸å˜. é’ˆå¯¹æ­¤é—®é¢˜ï¼Œä½œè€…æå‡ºäº†Language augmented CLIP (LaCLIP), åˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œé‡æ–°æè¿°è®­ç»ƒé›†çš„captionsï¼Œå¢åŠ æ–‡æœ¬çš„å¤šæ ·æ€§. å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸‹æ¸¸zero-shotä»»åŠ¡ä¸Šï¼Œæ€§èƒ½æœ‰æ˜æ˜¾æå‡.|<img src="./images/LaCLIP.png"  width="640px"/>| [[Github](https://github.com/LijieFan/LaCLIP)] <br> [[Paper](https://arxiv.org/pdf/2305.20088)] |
| [![Star](https://img.shields.io/github/stars/zjukg/Structure-CLIP.svg?style=social&label=Star)](https://github.com/zjukg/Structure-CLIP) <br> **Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations** <br>|CLIPåœ¨ç»“æ„åŒ–çš„æ–‡æœ¬-å›¾åƒåŒ¹é…ä¸Šè¡¨ç°ä¸å¤Ÿï¼Œå¦‚é€šè¿‡clip scoreå¹¶ä¸èƒ½åŒºåˆ«ä¸€å¼ å›¾æ˜¯äººå’¬ç‹—å’Œç‹—å’¬äºº. ä½œè€…è®¤ä¸ºé€ æˆè¿™ä¸ªé—®é¢˜çš„åŸå› æ˜¯CLIPåœ¨å­¦ä¹ å¤šæ¨¡æ€åœºæ™¯ä¸­çš„representationsæ—¶æœªèƒ½å……åˆ†åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†.  æ–‡ç« æå‡º Structure-CLIP ï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé€šè¿‡é›†æˆåœºæ™¯å›¾çŸ¥è¯†æ¥å¢å¼ºå¤šæ¨¡æ€ç»“æ„åŒ–è¡¨ç¤º. <br><br>ğŸ§Ÿâ€â™‚ï¸: 1.å¢åŠ éš¾ä¾‹è´Ÿæ ·æœ¬; 2. å®é™…å·¥ä½œä¸­ä¸ªäººä¹Ÿæƒ³è¿‡ç±»ä¼¼æ–¹æ³•ï¼Œå¤§æ¦‚æ˜¯é€šè¿‡åˆ†æè´Ÿè´£æè¿°çš„è¯æ€§ç­‰ï¼Œåˆ©ç”¨æœ€çŸ­ä¾èµ–è·¯å¾„ç­‰æ–¹æ³•æ‹†åˆ†å¥å­ï¼Œç„¶ååšenhance.| <img src="./images/Structure-CLIP.png"  width="640px"/> | [[Github](https://github.com/zjukg/Structure-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2305.06152)] |
| [![Star](https://img.shields.io/github/stars/mertyg/vision-language-models-are-bows.svg?style=social&label=Star)](https://github.com/mertyg/vision-language-models-are-bows) <br> **WHEN AND WHY VISION-LANGUAGE MODELS BEHAVE LIKE BAGS-OF-WORDS, AND WHAT TO DO ABOUT IT?** <br>|  å°½ç®¡VLMåœ¨ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ä¸é”™ï¼Œä½†å¯¹æ¨¡å‹å¦‚ä½•å­¦ä¹ ç±»åˆ«å’Œå±æ€§çš„ç»„åˆå…³ç³»ï¼Œç›®å‰å°šä¸æ¸…æ¥š.  æ–‡ç« é¦–å…ˆæå‡ºä¸€ä¸ªAttribution, Relation, and Order(ARO) çš„benchmarkï¼Œç³»ç»Ÿè¯„ä¼°VLMç†è§£ä¸åŒç±»å‹çš„å…³ç³»ã€å±æ€§å’Œé¡ºåºä¿¡æ¯çš„èƒ½åŠ›. ç„¶åå¯¹ä½¿ç”¨VLMå®ç°çš„æ£€ç´¢ä»»åŠ¡å’Œå¯¹æ¯”é¢„è®­ç»ƒåšäº†æ·±åˆ»è§£æï¼Œæå‡ºè§£é‡Šäº†å‡ ä¸ªé—®é¢˜. æœ€åæå‡ºNegCLIPæé«˜æ¨¡å‹å¯¹ attributes and relationsçš„ç†è§£.  <br><br>ğŸ§Ÿâ€â™‚ï¸: ICLR 2023 Oralï¼Œåˆçœ‹è§‰å¾—æ²¡ä»€ä¹ˆæ„æ€ï¼Œä½†ç²¾è¯»åæ”¶è·å¾ˆå¤š. æ¯”å¦‚å®é™…å·¥ä½œä¸­ï¼Œåˆ©ç”¨clipå®ç°å¤šæ¨¡æ€æ£€ç´¢ï¼Œè™½ç„¶clipä¸èƒ½å¾ˆå¥½çš„å­¦ä¹ objectå’Œå¯¹åº”å±æ€§çš„å…³ç³»ï¼Œä½†é€šè¿‡è§¦å‘key wordsï¼Œä¾æ—§å¯ä»¥è·å¾—å¾ˆå¥½çš„è¡¨ç°. æ–‡ç« å¯¹æ­¤åšäº†è§£è¯». ä»¥åŠNegCLIPï¼Œæ–¹æ³•ç›´æ¥ç®€å•ï¼Œå…¶å®å®é™…å·¥ä½œä¸­å¸¸ç”¨ç±»ä¼¼æ–¹æ³•. æ€»ä¹‹æ˜¯ä¸€ç¯‡å€¼å¾—ç»†å“çš„æ–‡ç« ğŸ‘| <img src= "./images/NegCLIP.png"  width="640px"/> | [[Github](https://github.com/mertyg/vision-language-models-are-bows)] <br> [[Paper](https://openreview.net/pdf?id=KRLUvxh8uaX)] |
|2024|
| [![Star](https://img.shields.io/github/stars/YichaoCai1/CLAP.svg?style=social&label=Star)](https://github.com/YichaoCai1/CLAP) <br> **CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts** <br>|ç›´æ¥ä½¿ç”¨[ä½œè€…çš„å›ç­”](https://www.zhihu.com/question/660698707/answer/3550999896)ï¼š ä»causalityç†è®ºå‡ºå‘ï¼ŒCLAPæ—¨åœ¨æå‡pretrained VLMåœ¨distribution shiftåœºæ™¯ä¸‹çš„generalizationèƒ½åŠ›ï¼ŒCLAPä»…éœ€åœ¨text modalityç”¨è¾ƒå°æˆæœ¬å»fine-tune CLIPæ¨¡å‹ï¼Œå¯ä»¥ä½¿pretrained representationsæ›´èšç„¦äºcontentï¼ˆobjectï¼‰æœ¬èº«ï¼Œä»è€Œæå‡æ¨¡å‹zero-shot/few-shotè¡¨ç°, ä»¥åŠdomain adaptationå’Œadversarial resilienceçš„èƒ½åŠ›.  |<img src="./images/CLAP.png"  width="640px"/>| [[Github](https://github.com/YichaoCai1/CLAP)] <br> [[Paper](https://arxiv.org/pdf/2311.16445)] |
| [![Star](https://img.shields.io/github/stars/apple/ml-tic-clip.svg?style=social&label=Star)](https://github.com/apple/ml-tic-clip) <br> **TIC-CLIP: CONTINUAL TRAINING OF CLIP MODELS** <br> |éšç€æ•°æ®çš„ä¸æ–­ç§¯ç´¯å’Œæ›´æ–°ï¼Œå¦‚ä½•ä½æˆæœ¬çš„è®­ç»ƒæ¨¡å‹ï¼Œä¸æœ€æ–°çš„æ•°æ®åŒæ­¥. æ–‡ç« æå‡ºä¸€ç§ç®€å•çš„rehearsal-basedçš„æ–¹æ¡ˆï¼Œä¸æ ‡å‡†çš„é¢„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œæé€Ÿ2.5x. | <img src="./images/TIC-CLIP.png"  width="640px"/>| [[Github](https://github.com/apple/ml-tic-clip)] <br> [[Paper](https://arxiv.org/pdf/2310.16226)] |
| [![Star](https://img.shields.io/github/stars/facebookresearch/MetaCLIP.svg?style=social&label=Star)](https://github.com/facebookresearch/MetaCLIP) <br> **DEMYSTIFYING CLIP DATA[MetaCLIP]** <br>|æ–‡ç« æ­ç¤ºäº†CLIPè®­ç»ƒæ•°æ®çš„ç®¡ç†æ–¹æ³•ï¼Œæå‡ºCLIPæ•°æ®ç®¡ç†ç®—æ³•ï¼Œæ¥ç®€åŒ–å’Œäº§ç”Ÿé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®.  å¹¶ä¸”ä»‹ç»äº†Metadata-Curated Language-Image Pre-training (MetaCLIP)---CLIP proç‰ˆ.  <br><br>ğŸ§Ÿâ€â™‚ï¸:å»ºè®®å¤§è§„æ¨¡æ•°æ®é›†ç®¡ç†é£Ÿç”¨ï¼Œæ•°æ®è´¨é‡æ¯”æ•°é‡é‡è¦å¾—å¤šï½| | [[Github](https://github.com/facebookresearch/MetaCLIP)] <br> [[Paper](https://arxiv.org/pdf/2309.16671)]|
| [![Star](https://img.shields.io/github/stars/dsam99/pc_clip.svg?style=social&label=Star)](https://github.com/dsam99/pc_clip) <br> **FINETUNING CLIP TO REASON ABOUT PAIRWISE DIFFERENCES** <br> |æ–‡ç« æå‡ºPC-CLIP (Pairwise Comparison CLIP)ï¼Œæé«˜CLIPæ¨ç†å·®å¼‚çš„èƒ½åŠ›. å…·ä½“è€Œè¨€ï¼Œå€ŸåŠ©LLMå¾—åˆ°å›¾ç‰‡ä¹‹é—´å·®å¼‚çš„æ–‡æœ¬æè¿°ï¼Œtext encoderå°†ä¸Šè¿°æè¿°ç¼–ç ï¼ŒåŒæ—¶image encoderåˆ†åˆ«ç¼–ç ä¸¤å¼ å›¾ç‰‡ï¼Œè®¡ç®—image embeddingçš„å·®å¼‚ï¼Œå¯¹é½ä¸¤ä¸ªæ¨¡æ€çš„å·®å¼‚. | <img src="./images/PC-CLIP.png"  width="640px"/>| [[Github](https://github.com/dsam99/pc_clip)] <br> [[Paper](https://arxiv.org/pdf/2409.09721)] |
| **FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models** <br> |å™ªå£°å’ŒCaptionçš„è´¨é‡å¯¹VLMæ¨¡å‹çš„è®­ç»ƒååˆ†é‡è¦. æ–‡ç« é¦–å…ˆç ”ç©¶åˆ†æäº†å½±å“è®­ç»ƒçš„ä¸¤ä¸ªå› ç´ ï¼šè´Ÿæ ·æœ¬çš„é”™è¯¯åˆ†é…ï¼Œä»¥åŠè´¨é‡çš„Captionå’Œå¤šæ ·æ€§è¾ƒä½. é’ˆå¯¹ä»¥ä¸Šé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒ-æ–‡æœ¬ã€å›¾åƒ-å›¾åƒå’Œæ–‡æœ¬-æ–‡æœ¬çš„positive pairçš„ç›¸ä¼¼æ€§æŒ–æ˜ç®—æ³•ï¼Œå‡å°‘è®­ç»ƒæ•°æ®ä¸­ç”±äºè¯­ä¹‰ç›¸ä¼¼çš„å›¾åƒæˆ–æ˜¯ç›¸ä¼¼æè¿°è€Œäº§ç”Ÿçš„false negativeçš„æ•°é‡.  é¢å¤–çš„ï¼Œä½œè€…å»ºè®®ä½¿ç”¨sigmoid lossè®­ç»ƒ.  | <img src="./images/FFF.png"  width="640px"/>| [[Paper](https://arxiv.org/pdf/2405.10286)] |


## Improvement & Innovation


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/FlagAI-Open/FlagAI.svg?style=social&label=Star)](https://github.com/FlagAI-Open/FlagAI) <br> **AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities** <br>|æ–‡ç« æå‡ºä¸€ä¸ªæ¦‚å¿µä¸Šç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥è®­ç»ƒä¸€ä¸ªå¼ºå¤§çš„åŒè¯­/å¤šè¯­å¤šæ¨¡æ€è¡¨ç¤ºæ¨¡å‹. ä½¿ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ–‡æœ¬ç¼–ç å™¨XLMRæ›¿æ¢Clipçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒæ¨¡å¼(Teacher Learning; Contrastive Learning)å¯¹é½æ–‡æœ¬å’Œå›¾åƒè¡¨å¾. | <img src="./images/AltCLIP.png"  width="640px"/> | [[Github](https://github.com/FlagAI-Open/FlagAI)] <br> [[Paper](https://arxiv.org/pdf/2211.06679)] |
|2023|
| [![Star](https://img.shields.io/github/stars/google-research/big_vision.svg?style=social&label=Star)](https://github.com/google-research/big_vision) <br> **CLIPPO: Image-and-Language Understanding from Pixels Only** <br>| æ–‡ç« å¯¹ä½¿ç”¨çº¯åŸºäºåƒç´ çš„æ¨¡å‹è¿›è¡Œæ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€å­¦ä¹ è¿›è¡Œæ¢ç´¢ã€‚CLIPPOæ˜¯ä¸€ä¸ªå•ç‹¬çš„è§†è§‰ Transformerï¼Œå®ƒå¤„ç†è§†è§‰è¾“å…¥æˆ–æ–‡æœ¬ï¼Œæˆ–ä¸¤è€…ä¸€èµ·ï¼Œæ‰€æœ‰éƒ½å‘ˆç°ä¸º RGB å›¾åƒï¼ˆæ–‡æœ¬åœ¨ç©ºç™½å›¾åƒä¸Šæ¸²æŸ“ï¼Œä½œä¸ºçº¯å›¾åƒå¤„ç†ï¼‰. æ‰€æœ‰æ¨¡æ€éƒ½ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å‚æ•°ï¼ŒåŒ…æ‹¬ä½çº§ç‰¹å¾å¤„ç†ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸å­˜åœ¨ç‰¹å®šäºæ¨¡æ€çš„åˆå§‹å·ç§¯ã€tokenization ç®—æ³•æˆ–è¾“å…¥åµŒå…¥è¡¨. CLIPPOä»…ç”¨ä¸€ä¸ªä»»åŠ¡è®­ç»ƒ--å¯¹æ¯”å­¦ä¹ . | <img src="./images/CLIPPO.png"  width="640px"/> | [[Github](https://github.com/google-research/big_vision)] <br> [[Paper](https://arxiv.org/pdf/2212.08045)] |
| [![Star](https://img.shields.io/github/stars/SunzeY/AlphaCLIP.svg?style=social&label=Star)](https://github.com/SunzeY/AlphaCLIP) <br> **Alpha-CLIP: A CLIP Model Focusing on Wherever You Want** <br>|Clipæ— æ³•å…³æ³¨åˆ°å±€éƒ¨åŒºåŸŸï¼Œé’ˆå¯¹æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºä¸€ä¸ªå¢å¼ºç‰ˆæœ¬çš„CLIPï¼Œåä¸ºAlpha-CLIP. Alpha-CLIPå¸¦æœ‰ä¸€ä¸ªè¾…åŠ©alphaé€šé“æ¥æç¤ºæ³¨æ„åŒºåŸŸï¼Œå¹¶é€šè¿‡æ„é€ æ•°ç™¾ä¸‡ä¸ªRGBAåŒºåŸŸ-æ–‡æœ¬å¯¹è¿›è¡Œäº†å¾®è°ƒã€‚Alpha-CLIPä¸ä»…ä¿ç•™äº†CLIPçš„è§†è§‰è¯†åˆ«èƒ½åŠ›ï¼Œè€Œä¸”å¯ä»¥ç²¾ç¡®æ§åˆ¶å›¾åƒå†…å®¹çš„é‡ç‚¹. å®ƒè¯æ˜äº†åœ¨å„ç§ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¼€æ”¾ä¸–ç•Œè¯†åˆ«ã€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œæ¡ä»¶2D /3Dç”Ÿæˆ. å®ƒå…·æœ‰å¼ºå¤§çš„æ½œåŠ›ï¼Œå¯ä½œä¸ºå›¾åƒç›¸å…³ä»»åŠ¡çš„é€šç”¨å·¥å…·. | <img src="./images/Alpha-CLIP.png"  width="640px"/> | [[Github](https://github.com/SunzeY/AlphaCLIP)] <br> [[Paper](https://arxiv.org/pdf/2312.03818)] |
| [![Star](https://img.shields.io/github/stars/OFA-Sys/Chinese-CLIP.svg?style=social&label=Star)](https://github.com/OFA-Sys/Chinese-CLIP) <br> **Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese** <br>|æ–‡ç« æå‡ºä¸­æ–‡CLIPé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ç»™å‡ºäº†ä¸¤é˜¶æ®µé¢„è®­ç»ƒæ³•: 1.å°†é¢„è®­ç»ƒClipçš„å›¾åƒç¼–ç å™¨å›ºå®šï¼Œä½¿ç”¨ä¸­æ–‡RoBERTAæ›¿æ¢æ–‡æœ¬ç¼–ç å™¨ï¼Œè®­ç»ƒRoBERTA; 2.æ–‡æœ¬ã€å›¾åƒç¼–ç å™¨åŒæ—¶è®­ç»ƒ.| <img src="./images/Chinese-CLIP.png"  width="640px"/> | [[Github](https://github.com/OFA-Sys/Chinese-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2211.01335)] |
| [![Star](https://img.shields.io/github/stars/baaivision/EVA.svg?style=social&label=Star)](https://github.com/baaivision/EVA) <br> **EVA-CLIP: Improved Training Techniques for CLIP at Scale** <br>|å¤§åŠ›å‡ºå¥‡è¿¹. | <img src="./images/EVA-CLIP.png"  width="640px"/> | [[Github](https://github.com/baaivision/EVA)] <br> [[Paper](https://arxiv.org/pdf/2303.15389)] |
|2024|
| [![Star](https://img.shields.io/github/stars/baaivision/EVA.svg?style=social&label=Star)](https://github.com/baaivision/EVA) <br> **EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters** <br>|å¤§åŠ›å‡ºå¥‡è¿¹. | <img src="./images/EVA-CLIP-18B.png"  width="640px"/> | [[Github](https://github.com/baaivision/EVA)] <br> [[Paper](https://arxiv.org/pdf/2402.04252)] |
| [![Star](https://img.shields.io/github/stars/xmed-lab/CLIP_Surgery.svg?style=social&label=Star)](https://github.com/xmed-lab/CLIP_Surgery) <br> **ACloser Look at the Explainability of Contrastive Language-Image Pre-training** <br>|æ–‡ç« å‘ç°äº†CLIPçš„å¯è§£é‡Šæ€§æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š1.å¯è§†åŒ–ç»“æœå’Œäººçš„æ„ŸçŸ¥æ˜¯åçš„ï¼›2.å¯è§†åŒ–æœ‰éå¸¸å¤šçš„å™ªå£°å“åº”. é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæ–‡ç« é˜è¿°äº†åŸå› ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªtrain-freeçš„è§£å†³æ–¹æ³•. <br><br>ğŸ§Ÿâ€â™‚ï¸:å·¥ä½œä¸Šé‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œä½¿ç”¨clipåšç›¸ä¼¼å¯¹å¯¹æ¯”ï¼Œcosç›¸ä¼¼åº¦åŸºæœ¬éƒ½åœ¨0.2+ï¼Œè¿™ç¯‡è®ºæ–‡ç»™äº†ç­”æ¡ˆï¼ŒåŒæ—¶Camå›¾çš„ç»“æœæå‡ä¹Ÿå¾ˆå¤§.ğŸ‘ | <img src="./images/CLIP_Surgery.png"  width="640px"/> | [[Github](https://github.com/xmed-lab/CLIP_Surgery)] <br> [[Paper](https://arxiv.org/pdf/2304.05653)]  [[çŸ¥ä¹](https://www.zhihu.com/question/595372017/answer/2982207851)]|
| [![Star](https://img.shields.io/github/stars/beichenzbc/Long-CLIP.svg?style=social&label=Star)](https://github.com/beichenzbc/Long-CLIP) <br> **Long-CLIP: Unlocking the Long-Text Capability of CLIP** <br>| CLIPçš„æ–‡æœ¬tokené•¿åº¦è¢«é™åˆ¶ä¸º77ï¼Œè€Œç ”ç©¶è¡¨æ˜å®é™…æœ‰æ•ˆé•¿åº¦ç”šè‡³ä¸åˆ°20. è¿™ä½¿å¾—CLIPæ— æ³•å¤„ç†è¯¦ç»†çš„æè¿°,é™åˆ¶äº†å…¶åœ¨å›¾åƒæ£€ç´¢å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢çš„åº”ç”¨. æœ¬æ–‡æå‡ºLong-CLIPä½œä¸ºCLIPçš„å³æ’å³ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒæ”¯æŒé•¿æ–‡æœ¬è¾“å…¥ï¼Œä¿ç•™ç”šè‡³è¶…è¶Šå…¶zero-shotçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è°ƒæ•´CLIPæ½œåœ¨ç©ºé—´ï¼Œä½¿å…¶æ˜“äºå–ä»£CLIPï¼Œè€Œæ— éœ€åœ¨ä¸‹æ¸¸æ¡†æ¶ä¸­è¿›è¡Œä»»ä½•è¿›ä¸€æ­¥çš„è°ƒæ•´.| <img src="./images/Long-CLIP.png"  width="640px"/> | [[Github](https://github.com/beichenzbc/Long-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2403.15378)]|
| [![Star](https://img.shields.io/github/stars/apple/ml-mobileclip.svg?style=social&label=Star)](https://github.com/apple/ml-mobileclip) <br> **MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training** <br>| æ–‡ç« æå‡ºäº†mobileç‰ˆçš„CLIPï¼Œä¸æ ‡å‡†çš„ViT-B/16 CLIPç›¸æ¯”ï¼Œé€Ÿåº¦æå‡2.3å€ï¼Œåœ¨38ä¸ªæµ‹è¯•é›†ä¸Šaccuracyå¹³å‡æé«˜2.9%. ä¸æ ‡å‡†CLIPç›¸æ¯”ï¼Œè®­ç»ƒæ•ˆç‡æå‡10-1000å€. ä¸»è¦çš„ä¸€äº›ç‚¹åŒ…æ‹¬: æ–‡æœ¬/å›¾åƒencoderçš„é‡æ–°é€‰æ‹©å’Œè®¾è®¡ã€å€ŸåŠ©CoCaå¯¹è®­ç»ƒé›†ç”Ÿæˆå¤šä¸ªcaptionè¿›è¡Œæ•°æ®é›†å¢å¼ºã€å¤šä¸ªå¤§æ¨¡å‹ï¼ˆCLIPï¼‰çš„Model ensemblingï¼Œä»¥åŠåŸºäºæ­¤è®¾è®¡çš„loss.| <img src="./images/MobileCLIP.png"  width="640px"/> | [[Github](https://github.com/apple/ml-mobileclip)] <br> [[Paper](https://arxiv.org/pdf/2311.17049)]|
| [![Star](https://img.shields.io/github/stars/deepglint/RWKV-CLIP.svg?style=social&label=Star)](https://github.com/deepglint/RWKV-CLIP) <br> **RWKV-CLIP: A Robust Vision-Language Representation Learner** <br>| æœ¬æ–‡ä»æ•°æ®å’Œæ¨¡å‹æ¶æ„çš„è§’åº¦è¿›ä¸€æ­¥æ¢è®¨äº† CLIP.  åŒæ—¶ä¸ºäº†è§£å†³å™ªå£°æ•°æ®çš„é—®é¢˜ï¼Œæé«˜ä»äº’è”ç½‘çˆ¬å–çš„å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬æ•°æ®çš„è´¨é‡ï¼Œä½œè€…å€ŸåŠ©LLMï¼Œæå‡ºäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„æè¿°ç”Ÿæˆæ¡†æ¶ï¼›æ­¤å¤–ï¼Œä½œè€…å°†RWKVå¼•å…¥VLMä¸­ï¼Œå°† Transformer çš„æœ‰æ•ˆå¹¶è¡Œè®­ç»ƒä¸ RNN çš„é«˜æ•ˆæ¨ç†ç›¸ç»“åˆ. å®éªŒè¯æ˜å…¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½.  <br><br>ğŸ§Ÿâ€â™‚ï¸: RWKVæ¶æ„æ˜¯å›½äººï¼ˆchineseï¼‰åŸåˆ›çš„ï¼Œé€šè¿‡é«˜æ•ˆçš„çº¿æ€§æ‰©å±•è§£å†³äº† Transformer ä¸­çš„å†…å­˜ç“¶é¢ˆå’ŒäºŒæ¬¡æ‰©å±•é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™äº†å¹¶è¡Œè®­ç»ƒå’Œå¼ºå¤§æ‰©å±•æ€§çš„è¡¨ç°ç‰¹æ€§. å…³äºRWKV å¯æŸ¥é˜…[RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/pdf/2305.13048)| <img src="./images/RWKV-CLIP1.png"  width="640px"/> <br> <img src="./images/RWKV-CLIP2.png"  width="640px"/> | [[Github](https://github.com/deepglint/RWKV-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2406.06973)]|
| [![Star](https://img.shields.io/github/stars/raytrun/mamba-clip.svg?style=social&label=Star)](https://github.com/raytrun/mamba-clip) <br> **CLIP-Mamba: CLIP Pretrained Mamba Models with OOD and Hessian Evaluation** <br>| åˆ©ç”¨CLIPæ–¹å¼è®­ç»ƒMamba . | | [[Github](https://github.com/raytrun/mamba-clip)] <br> [[Paper](https://arxiv.org/pdf/2404.19394)]|


## Data


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2024|
| [![Star](https://img.shields.io/github/stars/apple/ml-veclip.svg?style=social&label=Star)](https://github.com/apple/ml-veclip) <br> **VeCLIP: Improving CLIP Training via Visual-enriched Captions** <br>| é’ˆå¯¹ç½‘ç»œçˆ¬è™«çš„æ–‡æœ¬-å›¾åƒæ•°æ®å¯¹ï¼Œè¿›è¡Œcaptioné‡å†™ã€‚ä½¿ç”¨LLaVAç”Ÿæˆcaptionï¼Œç„¶åä¸çˆ¬è™«å¾—åˆ°çš„æè¿°ï¼ˆAltTextsï¼‰åšèåˆï¼Œé€å…¥Vicuna-1.1å¾—åˆ°é‡å†™åçš„caption.  |<img src="./images/VeCLIP.png"  width="640px"/>|  [[Github](https://github.com/apple/ml-veclip)] <br> [[Paper](https://arxiv.org/pdf/2310.07699)] |
| [![Star](https://img.shields.io/github/stars/hammoudhasan/SynthCLIP.svg?style=social&label=Star)](https://github.com/hammoudhasan/SynthCLIP) <br> **SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?** <br>| ä½¿ç”¨å…¨åˆæˆæ–‡æœ¬å›¾åƒå¯¹è®­ç»ƒ CLIP æ¨¡å‹ï¼Œä¸å…ˆå‰ä¾èµ–äºçœŸå®æ•°æ®çš„æ–¹æ³•æœ‰æ˜¾è‘—åŒºåˆ«ï¼ŒSynthCLIP å®ç°äº†ä¸åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„ CLIP æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½.  |<img src="./images/SynthCLIP.png"  width="640px"/>|  [[Github](https://github.com/hammoudhasan/SynthCLIP)] <br> [[Paper](https://arxiv.org/pdf/2402.01832)] |



## Distillation


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2023|
| [![Star](https://img.shields.io/github/stars/microsoft/Cream.svg?style=social&label=Star)](https://github.com/microsoft/Cream/tree/main/TinyCLIP) <br> **TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance** <br>|æ–‡ç« æå‡ºäº†ä¸€ç§é¢å‘å¤§è§„æ¨¡è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹çš„è·¨æ¨¡æ€è’¸é¦æ–¹æ³•:TinyCLIP. TinyClipåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯: affinity mimicking and weight inheritance. åŸºäºå¤šçº§æ¸è¿›å¼æ–¹æ¡ˆè¿›è¡Œaffinity mimickingå’ŒWeight inheritanceï¼Œå®ŒæˆClipæ¨¡å‹çš„å‹ç¼©åŠæ€§èƒ½ä¿çœŸï¼Œåœ¨é€Ÿåº¦å’Œå‡†ç¡®åº¦ä¸Šåšäº†è¾ƒå¥½çš„å¹³è¡¡. | <img src="./images/TinyCLIP.png"  width="640px"/> | [[Github](https://github.com/microsoft/Cream/tree/main/TinyCLIP)] <br> [[Paper](https://arxiv.org/pdf/2211.01335)] |
|2024|
| [![Star](https://img.shields.io/github/stars/winycg/CLIP-KD.svg?style=social&label=Star)](https://github.com/winycg/CLIP-KD) <br> **CLIP-KD: An Empirical Study of CLIP Model Distillation** <br>|æ–‡ç« æ ¸å¿ƒç›®çš„æ˜¯åˆ©ç”¨ä¸€ä¸ªå¤§å‹çš„æ•™å¸ˆCLIPæ¨¡å‹æ¥ç›‘ç£ä¸€ä¸ªå°å‹çš„å­¦ç”ŸCLIPæ¨¡å‹ï¼Œä½¿å¾—å­¦ç”ŸCLIPæ¨¡å‹å¯ä»¥åœ¨ä¿æŒè½»é‡çš„å‰æä¸‹æ˜¾è‘—æå‡æ€§èƒ½. æ–‡ç« ä»å…³ç³»ã€ç‰¹å¾ã€æ¢¯åº¦å’Œå¯¹æ¯”æ¨¡å¼çš„è§’åº¦æ¥æ£€éªŒCLIP-çŸ¥è¯†è’¸é¦çš„æœ‰æ•ˆæ€§ . æœ€åçš„æ¶ˆèå®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç®€å•çš„MSEè¿›è¡Œç‰¹å¾è’¸é¦å®ç°äº†æœ€å¥½çš„è’¸é¦æ€§èƒ½. | <img src="./images/CLIP-KD.png"  width="640px"/> | [[Github](https://github.com/winycg/CLIP-KD)] <br> [[Paper](https://arxiv.org/pdf/2307.12732)] |



## Loss


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/goel-shashank/CyCLIP.svg?style=social&label=Star)](https://github.com/goel-shashank/CyCLIP) <br> **CYCLIP: Cyclic Contrastive Language-Image Pretraining** <br>|Clipçš„ç›®æ ‡å‡½æ•°ä»…ä½¿ç”¨äº†è·¨æ¨¡æ€çš„å¯¹æ¯”lossï¼Œå¯¹äºå•ä¸ªæ¨¡æ€å†…éƒ¨å’Œè·¨æ¨¡æ€çš„i2tã€t2içš„å¯¹ç§°æ€§çº¦æŸç¨æ˜¾ä¸è¶³ï¼Œå¯èƒ½ä¼šå¯¼è‡´å›¾åƒå’Œæ–‡æœ¬ä¹‹å‰çš„inconsistent predictions. å¦‚æœå¯¹ç§°åŒ–ä¸¤ä¸ªä¸åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ä»¥åŠå›¾åƒ-å›¾åƒå¯¹å’Œæ–‡æœ¬-æ–‡æœ¬å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œåˆ™å¯ä»¥æ¶ˆé™¤å›¾åƒå’Œæ–‡æœ¬ç©ºé—´ä¸­çš„ä¸ä¸€è‡´ï¼ˆçœ‹å›¾ç‰‡æ›´å¥½ç†è§£ï¼‰. è®ºæ–‡æå‡ºäº†cross-modal consistencyå’Œin-modal consistencyä¸¤ç§lossï¼Œä¸æ ‡å‡†clipç›¸æ¯”ï¼Œåœ¨ä¸‹æ¸¸çš„zero-shotåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®ç‡æœ‰10% âˆ’ 24%çš„æå‡. | <img src="./images/AltCLIP.png"  width="640px"/> | [[Github](https://github.com/goel-shashank/CyCLIP)] <br> [[Paper](https://arxiv.org/pdf/2205.14459)] |
|2023|
| [![Star](https://img.shields.io/github/stars/google-research/big_vision.svg?style=social&label=Star)](https://github.com/google-research/big_vision) <br> **SigLip:Sigmoid Loss for Language Image Pre-Training** <br>|æ–‡ç« æå‡ºäº†ä¸€ç§ç”¨äºè¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆSigLIPï¼‰çš„ç®€å•æˆå¯¹ Sigmoid æŸå¤±. ä¸ä½¿ç”¨ Softmax å½’ä¸€åŒ–çš„æ ‡å‡†å¯¹æ¯”å­¦ä¹ ä¸åŒï¼ŒSigmoid æŸå¤±ä»…å¯¹å›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œæ“ä½œï¼Œå¹¶ä¸”ä¸éœ€è¦å¯¹å½’ä¸€åŒ–çš„æˆå¯¹ç›¸ä¼¼æ€§è¿›è¡Œå…¨å±€è§†å›¾.  Sigmoid æŸå¤±åŒæ—¶å…è®¸è¿›ä¸€æ­¥æ‰©å¤§æ‰¹é‡å¤§å°ï¼ŒåŒæ—¶åœ¨è¾ƒå°çš„æ‰¹é‡å¤§å°ä¸‹ä¹Ÿèƒ½è¡¨ç°æ›´å¥½. | <img src="./images/SigLip.png"  width="640px"/> | [[Github](https://github.com/google-research/big_vision)] <br> [[Paper](https://arxiv.org/pdf/2303.15343)] |



## Zero-Shot & Few-Shot & Classification


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2021|
| [![Star](https://img.shields.io/github/stars/gaopengcuhk/CLIP-Adapter.svg?style=social&label=Star)](https://github.com/gaopengcuhk/CLIP-Adapter) <br> **CLIP-Adapter: Better Vision-Language Models with Feature Adapters** <br>| CLIP Adapteræ˜¯ä¸€ä¸ªä¸ºfew-shot classficationä»»åŠ¡è®¾è®¡çš„ä¸€ä¸ªæ’å…¥å¼æ¨¡å—.åœ¨å†»ä½çš„clipç‰¹å¾ä¸Šæ·»åŠ ä¸€ä¸ªæ®‹å·®è¿æ¥çš„å¾®è°ƒå™¨ï¼Œä½¿å¾—CLIPèƒ½æ›´å¥½åœ°åº”å¯¹åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡.|<img src="./images/CLIP-Adapter.jpg"  width="640px"/>| [[Github](https://github.com/gaopengcuhk/CLIP-Adapter)] <br> [[Paper](https://arxiv.org/pdf/2110.04544)] |
| **CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification** <br>| åˆ©ç”¨CLIPåˆ†ç±». ä½œè€…è®¤ä¸ºCLIPçš„è®­ç»ƒä»…æ¶‰åŠå…¨å±€å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œå› æ­¤æ²¡æœ‰å¯¹å›¾åƒå—å’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´çš„ç»†ç²’åº¦å…³ç³»è¿›è¡Œå»ºæ¨¡. è¿™ç§å…³ç³»åœ¨ç²¾ç»†è®­ç»ƒçš„åˆ†ç±»ä»»åŠ¡ä¸­éå¸¸æœ‰ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åªæœ‰ä¸€å°éƒ¨åˆ†å›¾åƒå—æˆ–æ–‡æœ¬æ ‡è®°ä¸åˆ†ç±»ä»»åŠ¡ç›¸å…³çš„æƒ…å†µä¸‹. å¹¶ä¸”è®­ç»ƒé›†ä¸­ä¸å¯é¿å…çš„å­˜åœ¨å™ªå£°æ•°æ®.  é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œä½œè€…æå‡ºäº†Cross-Modality Attention CLIP (CMA-CLIP)ï¼Œå®éªŒä¸»è¦ä½¿ç”¨ç¤¾åª’å’Œç”µå•†é¢†åŸŸæ•°æ®é›†.  å…·ä½“çš„ï¼Œ1.æå‡ºsequence-wise æ¨¡å—æ¥æ•è·å›¾åƒå—å’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´çš„ç»†ç²’åº¦å…³ç³»ï¼Œåšæ³•å°±æ˜¯æŠŠclipç¼–ç åçš„å›¾åƒå’Œæ–‡æœ¬æ‹¼åˆ°ä¸€èµ·ç»„æˆæ–°çš„embeddingï¼Œé€åˆ°åç»­çš„ç½‘ç»œä¸­; 2.è®¾è®¡äº†modality-wise attentionæ¨¡å—ï¼Œå­¦ä¹ å„æ¨¡æ€çš„æƒé‡ï¼Œè¿™é‡Œç›´æ¥ç”¨äº†ã€ŠMultimodal Keyless Attention Fusion for Video Classificationã€‹ä¸­çš„Keyless Attentionï¼›3. task specific modality-wise attentionsï¼Œ å…¶å®å°±æ˜¯å¤šå¤´åšmulti-task.  æ•´ä½“æ²¡å•¥å¯è¯´çš„åœ°æ–¹.| <img src="./images/CMA-CLIP.png"  width="640px "/>| [[Paper](https://arxiv.org/pdf/2112.03562)] |
| [![Star](https://img.shields.io/github/stars/KeremTurgutlu/clip_art.svg?style=social&label=Star)](https://github.com/KeremTurgutlu/clip_art) <br> **CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification** <br>| æ–‡ç« ä¸»è¦è§£å†³ä¸¤ä¸ªæŒ‘æˆ˜ï¼šå®ä¾‹æ£€ç´¢å’Œç»†ç²’åº¦è‰ºæœ¯å“å±æ€§è¯†åˆ«. ä¸»è¦æ˜¯è®­ç»ƒæ•°æ®ç”Ÿæˆ+CLIPå¾®è°ƒï¼Œ æ²¡å¤ªå¤šå¯è¯´çš„ä¸œè¥¿.|<img src="./images/CLIP-Art.png"  width="640px"/>| [[Github](https://github.com/KeremTurgutlu/clip_art)] <br> [[Paper](https://openaccess.thecvf.com/content/CVPR2021W/CVFAD/papers/Conde_CLIP-Art_Contrastive_Pre-Training_for_Fine-Grained_Art_Classification_CVPRW_2021_paper.pdf)] |
|2022|
| [![Star](https://img.shields.io/github/stars/gaopengcuhk/Tip-Adapter.svg?style=social&label=Star)](https://github.com/gaopengcuhk/Tip-Adapter) <br> **Tip-Adapter: Training-free Adaption of CLIP** <br>|ä¸ºäº†æé«˜Clipçš„few-shotèƒ½åŠ›ï¼Œæ–‡ç« æå‡ºä¸€ç§å…è®­ç»ƒçš„æ–¹æ³•ï¼Œåä¸ºTip-Adapter. é€šè¿‡ä»å°‘æ ·æœ¬ç›‘ç£ä¸­æ„å»ºquery-keyç¼“å­˜æ¨¡å‹æ¥è·å–é€‚é…å™¨çš„æƒé‡. é€šè¿‡ç¼“å­˜æ¨¡å‹ï¼Œä¸ä¼ ç»Ÿçš„finetuneæ–¹æ³•ç›¸æ¯”ï¼ŒTip-Adapterè¡¨ç°å‡ºæé«˜çš„æ•ˆç‡. |<img src="./images/Tip-Adapter.png"  width="640px"/>| [[Github](https://github.com/gaopengcuhk/Tip-Adapter)] <br> [[Paper]( https://arxiv.org/pdf/2207.09519)]  |
| [![Star](https://img.shields.io/github/stars/ZiyuGuo99/CALIP.svg?style=social&label=Star)](https://github.com/ZiyuGuo99/CALIP) <br> **CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention** <br>| æ–‡ç« å‡ºå‘ç‚¹æ˜¯å¦‚ä½•åœ¨ä¸finetuneçš„æƒ…å†µä¸‹ï¼Œæå‡clipåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„zero-shotèƒ½åŠ›.æ–‡ç« æå‡ºäº†ä¸€ç§parameter-freeçš„æ³¨æ„åŠ›æ¨¡å—(CALIP)ï¼Œå¼•å¯¼è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºç›¸äº’äº¤äº’ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æ¢ç´¢è·¨æ¨¡å¼ä¿¡æ¯ç‰¹å¾.é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå›¾åƒä¸æ–‡æœ¬ä¸¤ä¸ªæ¨¡æ€ç‰¹å¾ç›¸äº’æ„ŸçŸ¥ï¼Œä»¥å®ç°æ›´å¥½çš„è‡ªé€‚åº”é›¶æ ·æœ¬å¯¹é½.|<img src="./images/CALIP.png"  width="640px"/>| [[Github](https://github.com/ZiyuGuo99/CALIP)] <br> [[Paper](https://arxiv.org/pdf/2209.14169)]  |
| [![Star](https://img.shields.io/github/stars/KaiyangZhou/CoOp.svg?style=social&label=Star)](https://github.com/KaiyangZhou/CoOp)   <br> **CoOp: Learning to Prompt for Vision-Language Models** <br>| å—NLPé¢†åŸŸprompt learningçš„å¯å‘ï¼Œæ–‡ç« æå‡ºäº†Context Optimization(CoOp)ï¼Œç”¨äºå°†ç±»CLIPå¼çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿ç§»åˆ°ä¸‹æ¸¸å›¾åƒè¯†åˆ«ä»»åŠ¡.å…·ä½“è€Œè¨€ï¼ŒCoOpå°†é¢„è®­ç»ƒæ¨¡å‹å‚æ•°freezeï¼Œä½¿ç”¨å¯å­¦ä¹ å‘é‡å¯¹æç¤ºçš„ä¸Šä¸‹æ–‡å•è¯è¿›è¡Œå»ºæ¨¡.ä½œè€…åœ¨11ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸ŠéªŒè¯CoOpçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºCoOpçš„æ€§èƒ½æ˜æ˜¾å¥½äºåŸå§‹é¢„è®­ç»ƒæ¨¡å‹å¦‚CLIP.|<img src="./images/CoOp.png"  width="640px"/>| [[Github](https://github.com/KaiyangZhou/CoOp)] <br> [[Paper](https://arxiv.org/pdf/2109.01134)] |
| [![Star](https://img.shields.io/github/stars/KaiyangZhou/CoOp.svg?style=social&label=Star)](https://github.com/KaiyangZhou/CoOp)   <br> **CoCoOp: Conditional Prompt Learning for Vision-Language Models** <br>|é’ˆå¯¹CoOpæ³›åŒ–æ€§å·®çš„é—®é¢˜ï¼Œå³:å­¦ä¹ åˆ°çš„ä¸Šä¸‹æ–‡å¯¹æ•°æ®é›†ä¸­unseen classesçš„æ³›åŒ–æ€§ä¸å¥½.æ–‡ç« æå‡ºConditional Context Optimization (CoCoOp)ï¼Œåœ¨CoOpåŸºç¡€ä¸Šï¼Œå¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„ç½‘ç»œï¼Œåä¸ºMeta-Net:ä¸ºæ¯å¼ å›¾åƒç”Ÿæˆinput-conditional tokens. input-conditional tokensä¸ CoOpä¸­çš„learnable vectorså åŠ ï¼Œå…±åŒå‚ä¸è®­ç»ƒ.å¤§é‡å®éªŒè¡¨æ˜ï¼Œå¯¹äºunseen classesï¼ŒCoCoOp æ¯” CoOp çš„æ³›åŒ–èƒ½åŠ›è¦å¥½å¾—å¤šï¼Œç”šè‡³æ˜¾ç¤ºå‡ºè¶…è¶Šå•ä¸ªæ•°æ®é›†çš„å¯è¿ç§»æ€§ï¼Œ å¹¶äº§ç”Ÿæ›´å¼ºçš„é¢†åŸŸæ³›åŒ–æ€§èƒ½ |<img src="./images/CoCoOp.png"  width="640px"/>| [[Github](https://github.com/KaiyangZhou/CoOp)] <br> [[Paper](https://arxiv.org/pdf/2203.05557)] |
| [![Star](https://img.shields.io/github/stars/LightDXY/FT-CLIP.svg?style=social&label=Star)](https://github.com/LightDXY/FT-CLIP)   <br> **CLIP Itself is a Strong Fine-tuner:Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet** <br>| æ–‡ç« é€šè¿‡ä¸€ç³»åˆ—è¯•éªŒï¼ŒéªŒè¯ä½¿ç”¨ä¸åŒè¶…å‚æ•°finetune clipååœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡çš„è¡¨ç°.  |<img src="./images/FT-CLIP.png"  width="640px"/>| [[Github](https://github.com/LightDXY/FT-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2212.06138)] |
|[![Star](https://img.shields.io/github/stars/xmed-lab/CLIPN.svg?style=social&label=Star)](https://github.com/xmed-lab/CLIPN)   <br> **CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No** <br>| æ–‡ç« çš„motivationæ˜¯é€šè¿‡å‘CLIPæä¾›positiveçš„è¯­ä¹‰æç¤ºå’Œnegativeçš„è¯­ä¹‰æç¤ºï¼Œä»¥æ­¤è®©CLIPæ‹¥æœ‰åŒºåˆ†OODï¼ˆOut-of-distributionï¼‰å’ŒIDï¼ˆin-distributionï¼‰æ ·æœ¬çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´,æ–‡ç« è®¾è®¡äº†ä¸€ä¸ªå¯å­¦ä¹ çš„"å¦å®š"æç¤ºåŠä¸€ä¸ªé’ˆå¯¹"å¦å®š"çš„æ–‡æœ¬ç¼–ç å™¨,ä»¥æ•æ‰å›¾åƒä¸­çš„å¦å®šè¯­ä¹‰.|<img src="./images/CLIPN.png"  width="640px"/>| [[Github](https://github.com/xmed-lab/CLIPN)] <br> [[Paper](https://arxiv.org/pdf/2308.12213v2)] |
|[![Star](https://img.shields.io/github/stars/muzairkhattak/multimodal-prompt-learning.svg?style=social&label=Star)](https://github.com/muzairkhattak/multimodal-prompt-learning)   <br> **MaPLe: Multi-modal Prompt Learning** <br>|è¯¸å¦‚CLIPçš„VLMæ¨¡å‹ï¼Œå¯¹æ–‡æœ¬æç¤ºå¾ˆæ•æ„Ÿï¼Œéœ€è¦ä»”ç»†é€‰æ‹©promptæ‰èƒ½å‘æŒ¥è‰¯å¥½ä½œç”¨.  é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ï¼Œè¿‘æœŸæ¯”è¾ƒå¸¸ç”¨å­¦ä¹ æç¤ºä½œä¸ºæ–‡æœ¬è¾“å…¥çš„æ–¹æ¡ˆï¼ˆlearn promptsï¼‰å¾®è°ƒCLIP.  ä½œè€…è®¤ä¸ºåªåœ¨å•ä¸ªbranchä¸­ä½¿ç”¨æç¤ºæ¥è°ƒæ•´ CLIPçš„è¡¨ç¤ºå¹¶ä¸æ˜¯æœ€ä¼˜çš„ï¼Œå› ä¸ºå®ƒä¸å…è®¸çµæ´»åœ°åŠ¨æ€è°ƒæ•´ä¸‹æ¸¸ä»»åŠ¡çš„ä¸¤ä¸ªè¡¨ç¤ºç©ºé—´.  ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†MaPLeï¼Œåœ¨imageå’Œtextä¸¤ä¸ªåˆ†æ”¯éƒ½è¿›è¡Œprompt learningï¼Œæé«˜ä¸¤ä¸ªæ¨¡æ€çš„è¡¨å¾ä¸€è‡´æ€§. ä¸CoCoOp ç›¸æ¯”ï¼ŒMaPLe è¡¨ç°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ï¼Œåœ¨ 11 ä¸ªä¸åŒçš„å›¾åƒè¯†åˆ«æ•°æ®é›†æ€§èƒ½å¹³å‡æ˜æ˜¾æå‡. |<img src="./images/MaPLe.png"  width="640px"/>| [[Github](https://github.com/muzairkhattak/multimodal-prompt-learning)] <br> [[Paper](https://arxiv.org/pdf/2210.03117)] |
|2023|
| [![Star](https://img.shields.io/github/stars/linyq2117/TagCLIP.svg?style=social&label=Star)](https://github.com/linyq2117/TagCLIP) <br> **TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training** <br>|CLIPåœ¨å¼€æ”¾è¯æ±‡åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼—ï¼Œä½†åœ¨å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡è¡¨ç°ä¸è¶³ï¼Œå› ä¸ºCLIPçš„å›¾åƒç¼–ç å™¨ä¸­çš„tokenç»è¿‡è®­ç»ƒä»¥æ•è·å…¨å±€ç‰¹å¾ï¼Œä»¥æ­¤åŒºåˆ†ç”±å¯¹æ¯”æŸå¤±ç›‘ç£çš„ä¸åŒæ–‡æœ¬æè¿°ï¼Œè¿™å¯¹äºå•æ ‡ç­¾åˆ†ç±»éå¸¸æœ‰æ•ˆ. å¤šæ ‡ç­¾åˆ†ç±»æ°æ°ç›¸åï¼Œå…¨å±€ç‰¹å¾å¾€å¾€ç”±æœ€çªå‡ºçš„ç±»åˆ«ä¸»å¯¼ï¼Œè€Œ softmax æ“ä½œçš„å¯¹æ¯”æ€§è´¨åŠ å‰§äº†è¿™ç§æƒ…å†µ.  é’ˆå¯¹ä»¥ä¸Šé—®é¢˜ï¼Œä½œè€…æå‡ºTagCLIP ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæå‡CLIP çš„å¤šæ ‡ç­¾åˆ†ç±»èƒ½åŠ›.  å…·ä½“è€Œè¨€ï¼Œ1. ä½œè€…å‡è®¾å¹¶å®éªŒè¯æ˜ï¼ŒCLIPçš„å›¾åƒç¼–ç å™¨æœ€åä¸€å±‚çš„attentionæ“ä½œç ´åäº†ç©ºé—´ä¿¡æ¯ï¼Œå³å‰é¢çš„CLIPç‰¹å¾ä¿æœ‰spatial information ï¼Œå› æ­¤TagCLIPå¿½ç•¥æœ€åä¸€ä¸ªself-attentionæ“ä½œæ¥ç»´æŒspatial information.  2.æå‡ºä¸€ç§local-to-global çš„æ–¹æ³•ï¼Œå³é¦–å…ˆç»è¿‡patchçº§çš„åˆ†ç±»å¾—åˆ°ç²—æ ‡ç­¾(å¾ˆå·§å¦™ï¼Œè¾“å…¥çš„text promptä¸ºæ ‡ç­¾ï¼ŒCLIPçš„text encoderå½“ä½œåˆ†ç±»å™¨ï¼ŒåŸºäºcontrastive losså¾—åˆ°çš„ç›¸ä¼¼åº¦çŸ©é˜µå³ä¸ºscore)ï¼Œç„¶åä½¿ç”¨dual-masking attention refinementï¼ˆDMARï¼‰ä¼˜åŒ–åˆ†ç±»scoreï¼ˆç›´æ¥å€ŸåŠ©Vitï¼Œå› ä¸ºself-attentionå­¦ä¹ äº†å›¾åƒpatchä¹‹é—´çš„å…³ç³»ï¼Œå¯ç›´æ¥ç”¨æ¥refine patchä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼‰ï¼Œæœ€åç»è¿‡class-wise reidentification (CWR) æ¨¡å—å¾—åˆ°æœ€ç»ˆç»“æœï¼ˆpatchå¯¹åº”åŸå›¾åŒºåŸŸé€åˆ°CLIPï¼Œå˜æˆä¸€ä¸ªå•æ ‡ç­¾åˆ†ç±»é—®é¢˜ï¼Œåšdouble checkï¼‰.|<img src="./images/TagCLIP.png"  width= "640px"/>| [[Github](https://github.com/linyq2117/TagCLIP)] <br> [[Paper](https://arxiv.org/pdf/2312.12828)] |
| [![Star](https://img.shields.io/github/stars/linzhiqiu/cross_modal_adaptation.svg?style=social&label=Star)](https://github.com/linzhiqiu/cross_modal_adaptation) <br> **Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models** <br>| æ–‡ç« é˜è¿°äº†ä¸€ä¸ªè§‚ç‚¹ï¼Œäººç±»å¯ä»¥é€šè¿‡å­¦ä¹ ä¸€ä¸ªç‰©ä½“çš„å¤šä¸ªç»´åº¦çŸ¥è¯†ï¼Œæ¥å¢å¼ºå¯¹è¯¥ç‰©ä½“çš„è§†è§‰ç†è§£. å¦‚äººä»¬å¯ä»¥é€šè¿‡é˜…è¯»æœ‰å…³ç‹—çš„å†…å®¹å¹¶å¬å®ƒä»¬çš„å«å£°æ¥åœ¨è§†è§‰ç»´åº¦åŒºåˆ†ç‹—.  åŸºäºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è·¨æ¨¡å¼ä½¿ç”¨çš„ç®€å•ç­–ç•¥ï¼šå°†æ¥è‡ªä¸åŒæ¨¡æ€çš„æ ·æœ¬åšä¸ºé¢å¤–çš„å°æ ·æœ¬æ•°æ®.  æœ¬è´¨ä¸Šå°±æ˜¯å°†n-shotçš„é—®é¢˜è½¬ä¸ºï¼ˆn+1ï¼‰shot. è¿™ç§ç®€å•çš„ç­–ç•¥å¯ä»¥é€šè¿‡ç®€å•çš„çº¿æ€§åˆ†ç±»å™¨äº§ç”Ÿå…¨é¢çš„ SOTA ç»“æœ.  <br><br>ğŸ§Ÿâ€â™‚ï¸:  çœ‹å®Œè¿™ç¯‡æ–‡ç« æœ‰ç‚¹è¿·ç³Šï¼Œä¸çŸ¥é“åœ¨å¹²å˜›...ä¸ªäººç†è§£è™½ç„¶æ•°æ®æ˜¯æ¥è‡ªä¸åŒæ¨¡æ€ï¼Œä½†å…¶æè¿°çš„æœ¬è´¨éƒ½æ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Œè€Œä¾‹å¦‚CLIPè¿™æ ·çš„VLMæ¨¡å‹ï¼Œå…¶æœ¬è´¨å°±æ˜¯å°†ä¸åŒæ¨¡æ€ç‰¹å¾æ˜ å°„åˆ°ä¸€ä¸ªç©ºé—´ä¸Šï¼Œæ‰€ä»¥è™½ç„¶æ˜¯ä¸åŒæ¨¡æ€çš„æ•°æ®ï¼Œä½†å…¶æœ¬è´¨æ˜¯ä¸€ä¸ª. å› æ­¤æ–‡ç« å°†ä¸åŒæ¨¡æ€çš„æ ·æœ¬åšä¸ºé¢å¤–çš„å°æ ·æœ¬æ•°æ® ï¼Œæœ¬è´¨å°±æ˜¯å¢åŠ äº†å°æ ·æœ¬æ•°æ®ï¼Œå› æ­¤å¯ä»¥æå‡è¡¨ç°. ä¸ªäººç²—æµ…çš„ç†è§£. |<img src="./images/cross_modal_adaptation.png"  width= "640px"/>| [[Github](https://github.com/linzhiqiu/cross_modal_adaptation)] <br> [[Paper](https://arxiv.org/pdf/2301.06267)] |
|2024|
| [![Star](https://img.shields.io/github/stars/TJU-sjyj/AMU-Tuning.svg?style=social&label=Star)](https://github.com/TJU-sjyj/AMU-Tuning) <br> **AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning** <br>| æå‡ºäº†ä¸€ç§åä¸ºAMU-Tuningçš„æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›åŸºäºCLIPæ¨¡å‹çš„å°æ ·æœ¬å­¦ä¹ æ€§èƒ½. è¯¥æ–¹æ³•é€šè¿‡åˆ†æå…³é”®ç»„ä»¶â€”â€”logitç‰¹å¾ã€logité¢„æµ‹å™¨å’Œlogitèåˆâ€”â€”æ¥å­¦ä¹ æœ‰æ•ˆçš„logitåå·®ï¼Œå¹¶é€šè¿‡åˆ©ç”¨è¾…åŠ©ç‰¹å¾ã€å¤šåˆ†æ”¯è®­ç»ƒçš„ç‰¹å¾åˆå§‹åŒ–çº¿æ€§åˆ†ç±»å™¨ä»¥åŠåŸºäºä¸ç¡®å®šæ€§çš„èåˆç­–ç•¥ï¼Œå°†logitåå·®æœ‰æ•ˆåœ°æ•´åˆåˆ°CLIPä¸­ï¼Œä»¥æé«˜å°æ ·æœ¬åˆ†ç±»çš„å‡†ç¡®æ€§. |<img src="./images/AMU-Tuning.png"  width="640px"/>| [[Github](https://github.com/TJU-sjyj/AMU-Tuning)] <br> [[Paper](https://arxiv.org/pdf/2404.089588)] |
| [![Star](https://img.shields.io/github/stars/SegoleneMartin/transductive-CLIP.svg?style=social&label=Star)](https://github.com/SegoleneMartin/transductive-CLIP) <br> **Transductive Zero-Shot and Few-Shot CLIP** <br>| å°†è½¬å¯¼æ¨ç†ï¼ˆTransductive Inferenceï¼‰å¼•å…¥åˆ°VLMä¸­ï¼Œæ”¹è¿›å°æ ·æœ¬åˆ†ç±»é—®é¢˜.  å…·ä½“åšæ³•æ˜¯åˆ©ç”¨æ”¯æŒé›†ï¼ˆsupport setï¼‰ä¸­å¯ç”¨çš„ç›‘ç£ï¼ˆlabelï¼‰æ¥é¢„æµ‹æŸ¥è¯¢æ ·æœ¬ï¼ˆqueryï¼‰çš„ç±»åˆ«.  <br><br>ğŸ§Ÿâ€â™‚ï¸: è½¬å¯¼æ¨ç†ï¼ˆTransductive Inferenceï¼‰æ˜¯ä¸€ç§é€šè¿‡è§‚å¯Ÿç‰¹å®šçš„è®­ç»ƒæ ·æœ¬ï¼Œè¿›è€Œé¢„æµ‹ç‰¹å®šçš„æµ‹è¯•æ ·æœ¬çš„æ–¹æ³•.  æ–‡ç« ä¸­çš„å…¬å¼å¾ˆå¤šï¼Œä¸ªäººæ²¡æœ‰å®Œå…¨è¯»æ‡‚... |<img src="./images/transductive-CLIP.png"  width="640px"/>| [[Github](https://github.com/SegoleneMartin/transductive-CLIP)] <br> [[Paper](https://hal.science/hal-04534868v1/document)] |
| [![Star](https://img.shields.io/github/stars/The-Shuai/DeIL.svg?style=social&label=Star)](https://github.com/SegoleneMartin/The-Shuai/DeIL) <br> **DeIL : Direct-and-Inverse CLIP for Open-World Few-Shot Learning** <br>| é’ˆå¯¹å¼€æ”¾ä¸–ç•Œå°‘æ ·æœ¬å­¦ä¹ -Open-World Few-Shot Learning (OFSL)é—®é¢˜ï¼Œä½œè€…å¼•å…¥Direct-and-Inverseçš„æ¦‚å¿µ ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•DeILï¼Œå€ŸåŠ©CLIPè§£å†³OFSLé—®é¢˜.  DeILç”±ä»¥ä¸‹å‡ ä¸ªæ¨¡å—æ„æˆï¼Œ1. DeIL-Pretrainerï¼šä½¿ç”¨CLIPNè®¡ç®—ç›®æ ‡å›¾ç‰‡ä¸å±äºlabelç±»åˆ«çš„æ¦‚ç‡ï¼ˆInverse-phaseï¼‰ï¼Œæ‰¾åˆ°å™ªå£°å›¾ç‰‡ï¼Œç„¶ååˆ©ç”¨CLIPä¿®æ­£å™ªå£°å›¾ç‰‡çš„labelï¼ˆDirect-phaseï¼‰ ; 2. æ•°æ®å¢å¼ºæ¨¡å—ï¼šå€ŸåŠ©DALL-E ï¼Œå¯¹æ”¹æ­£åçš„è®­ç»ƒæ•°æ®é›†åšå¢å¼º; 3. DeIL-Adapter ï¼šCLIP+Adaptoråˆ†ç±»å™¨ï¼ŒlossåŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼Œåˆ†ç±»å™¨ç›´æ¥é¢„æµ‹çš„åˆ†ç±»æŸå¤±ä»¥åŠä½¿ç”¨ CLIPN çš„é€†å‘æ¨ç†åé€‰æ‹©çš„è´Ÿæ ·æœ¬æ­£æ ·æœ¬çš„å¯¹æ¯”æŸå¤±.  æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ¨ç†é˜¶æ®µï¼Œä»…éœ€è¦ DeIL-Adapter å†…çš„åˆ†ç±»å™¨.... <br><br>ğŸ§Ÿâ€â™‚ï¸:  çœŸçš„ç»•|<img src="./images/DeIL.png"  width="640px"/>| [[Github](https://github.com/SegoleneMartin/The-Shuai/DeIL)] <br> [[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Shao_DeIL_Direct-and-Inverse_CLIP_for_Open-World_Few-Shot_Learning_CVPR_2024_paper.pdf)] |



## Retrieval


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2021|
| [![Star](https://img.shields.io/github/stars/ArrowLuo/CLIP4Clip.svg?style=social&label=Star)](https://github.com/ArrowLuo/CLIP4Clip) <br> **CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval** <br>| æ–‡ç« æå‡ºäº† CLIP4Clip æ¨¡å‹ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å°† CLIP æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°è§†é¢‘è¯­è¨€æ£€ç´¢ä»»åŠ¡ä¸Šï¼Œç ”ç©¶äº†ä¸‰ç§ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼šæ— å‚æ•°å‹ã€é¡ºåºå‹å’Œç´§å¯†å‹.  æ–‡ç« ä»å¤§é‡çš„å®éªŒä¸­å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š1. å¯¹äºè§†é¢‘æ–‡æœ¬æ£€ç´¢ä»»åŠ¡ï¼Œåªç”¨ä¸€å¼ å›¾åƒæ¥è¿›è¡Œè§†é¢‘ç¼–ç æ˜¯ä¸å¤Ÿï¼›2.  åœ¨å¤§è§„æ¨¡çš„è§†é¢‘-æ–‡æœ¬è®­ç»ƒé›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥æé«˜æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé›¶æ ·æœ¬é¢„æµ‹çš„èƒ½åŠ›ï¼›3.ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPï¼Œå¯¹äºå°æ•°æ®é›†ï¼Œæœ€å¥½ä¸è¦å¼•å…¥æ–°å‚æ•°ï¼Œè§†é¢‘å¸§é‡‡ç”¨å¹³å‡æ± åŒ–çš„æ–¹å¼ï¼›å¯¹äºå¤§æ•°æ®é›†ï¼Œæœ€å¥½å¼•å…¥æ›´å¤šå‚æ•°ï¼Œä»¥å­¦ä¹ å¤§å‹æ•°æ®é›†çš„æ—¶é—´ä¾èµ–æ€§ï¼› 4.ä½œè€…ç»™å‡ºäº†æœ€ä½³è¶…å‚æ•°è®¾ç½®. |<img src="./images/CLIP4Clip.png"  width="640px"/>  | [[Github](https://github.com/ArrowLuo/CLIP4Clip)] <br> [[Paper](https://arxiv.org/pdf/2104.08860)] |
|2022|
| [![Star](https://img.shields.io/github/stars/mzhaoshuai/CenterCLIP.svg?style=social&label=Star)](https://github.com/mzhaoshuai/CenterCLIP) <br> **CenterCLIP: Token Clustering for Efficient Text-Video Retrieval** <br>| åˆ©ç”¨CLIPè¿›è¡Œæ–‡æœ¬-è§†é¢‘æ£€ç´¢ä»»åŠ¡ï¼Œç”±äºè§†é¢‘æ—¶åŸŸçš„è¿ç»­æ€§ï¼Œä¼šäº§ç”Ÿå¾ˆå¤šåŒè´¨åŒ–çš„tokenï¼Œå¢åŠ äº†è®¡ç®—æˆæœ¬. ä¸ºäº†å‡å°‘å†—ä½™è§†é¢‘tokençš„æ•°é‡ï¼Œä½œè€…æå‡ºa multi-segment token clusteringç®—æ³•ï¼Œæ‰¾åˆ°æœ€å…·ä»£è¡¨æ€§çš„tokenå¹¶ä¸¢å¼ƒéå¿…è¦çš„. å…·ä½“è€Œè¨€ï¼Œå°†è§†é¢‘åˆ†ç‰‡ï¼Œæ¯ç‰‡å•ç‹¬èšç±»ï¼Œä¸”åªä¿ç•™ç°‡çš„center tokens. å°†æ‰€æœ‰center tokensæ‹¼æ¥ç»„æˆæ–°çš„visual sequenceé€å¦‚å…¥transformerè¿›è¡Œè®­ç»ƒ. |<img src="./images/CenterCLIP.png"  width="640px"/>  | [[Github](https://github.com/mzhaoshuai/CenterCLIP)] <br> [[Paper](https://arxiv.org/pdf/2205.00823)] |
|2023|
| [![Star](https://img.shields.io/github/stars/ABaldrati/CLIP4Cir.svg?style=social&label=Star)](https://github.com/ABaldrati/CLIP4Cir) <br> **Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features** <br>| ä½¿ç”¨clipè¿›è¡Œæ£€ç´¢. åˆ†ä¸ºä¸¤æ­¥ï¼š1.å¾®è°ƒclipçš„text encoder å’Œimage encoderï¼›2.è®¾è®¡ä¸€ä¸ªcombinerï¼Œå°†ä¸¤ä¸ªæ¨¡æ€ç‰¹å¾fusionï¼Œç”¨è¿™ä¸ªç‰¹å¾åšretrieval. |<img src="./images/CLIP4Cir1.png"  width="640px"/>   <img src="./images/CLIP4Cir2.png"  width="640px"/>| [[Github](https://github.com/ABaldrati/CLIP4Cir)] <br> [[Paper](https://arxiv.org/pdf/2308.11485)] | 
| [![Star](https://img.shields.io/github/stars/aneeshan95/Sketch_LVM.svg?style=social&label=Star)](https://github.com/aneeshan95/Sketch_LVM) <br> **CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not** <br>| å°†CLIPåº”ç”¨äºé›¶æ ·æœ¬è‰å›¾æ£€ç´¢ä»»åŠ¡ï¼ˆZS-SBIRï¼‰,é’ˆå¯¹ç»†ç²’åº¦çš„æ£€ç´¢åšå‡ºæ”¹è¿›. ä¸»è¦åŒ…æ‹¬1. prompt learning; 2. å¼•å…¥ä¸€ä¸ªæ­£åˆ™é¡¹ï¼Œä½¿è·¨ç±»åˆ«ï¼ˆæ•°æ®é›†ï¼‰çš„ç›¸å¯¹è·ç¦»ä¸€è‡´ï¼ˆæœ€å°åŒ–æ¯ä¸ªç±»åˆ«å¯¹ä¹‹é—´çš„ç›¸å¯¹è·ç¦»åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦ï¼‰ï¼›3. patch shufflingï¼Œè‰å›¾-å›¾ç‰‡æ•°æ®å¯¹åˆ†æˆNxNçš„patchï¼Œéšæœºåšç›¸åŒçš„shuffle. ä½œè€…è®¤ä¸ºæ‰“ä¹±çš„è‰å›¾åº”è¯¥æ›´æ¥è¿‘å…·æœ‰ç›¸åŒæ’åˆ—é¡ºåºæ‰“ä¹±çš„å›¾ç‰‡ï¼Œè¿œç¦»ä¸åŒæ’åˆ—çš„å›¾ç‰‡. è¿™ç§permutation-invariance æœ‰åŠ©äºæ¨¡å‹ç»†ç²’åº¦çš„ç†è§£.  é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæˆ‘ä¸ä¹‹å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ç›¸æ¯”ï¼Œæ€§èƒ½æ˜¾ç€æé«˜äº† 26.9%.| <img src="./images/Sketch_LVM.png"  width="640px"/>| [[Github](https://github.com/aneeshan95/Sketch_LVM)] <br> [[Paper](https://arxiv.org/pdf/2303.13440)] |
|2024|
| **JINA CLIP: Your CLIP Model Is Also Your Text Retriever** <br>| ä¼ ç»Ÿçš„text embeddingæ¨¡å‹ï¼Œåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬æ£€ç´¢ä¸­å‡ºè‰²ï¼Œä½†æ— æ³•æ‰§è¡Œcross-modalä»»åŠ¡. è¯¸å¦‚Clipä¹‹ç±»çš„æ¨¡å‹ï¼Œæœ‰æ•ˆåœ°å¯¹é½å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ï¼Œä½†ç”±äºå…¶è®­ç»ƒæ–¹æ³•å’Œä¸Šä¸‹æ–‡é™åˆ¶ï¼Œå› æ­¤æœªé’ˆå¯¹æ–‡æœ¬åˆ°æ–‡æœ¬æ£€ç´¢è¿›è¡Œä¼˜åŒ–. æ–‡ç« æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šä»»åŠ¡å¯¹æ¯”è®­ç»ƒæ–¹æ³•ï¼Œåœ¨å•ä¸ªæ¨¡å‹ä¸­å®ç°äº†state-of-the-artçš„æ–‡æœ¬åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢èƒ½åŠ›. |<img src="./images/JINA-CLIP.png"  width="640px"/>   | [[huggingface](https://huggingface.co/jinaai/jina-clip-v1)] <br> [[Paper](https://arxiv.org/pdf/2405.20204))] |


## Detection


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/microsoft/RegionCLIP.svg?style=social&label=Star)](https://github.com/microsoft/RegionCLIP) <br> **RegionCLIP: Region-based Language-Image Pretraining** <br>|è™½ç„¶CLIPåœ¨zero-shotå’Œtransfer learning settingsè¿™æ ·çš„å›¾ç‰‡åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä¿—ï¼Œä½†å¦‚æœç›´æ¥å°†CLIPåº”ç”¨åˆ°ç›®æ ‡æ£€æµ‹è¿™ç§è¯†åˆ«å›¾åƒåŒºåŸŸçš„ä»»åŠ¡ä¸Šï¼Œè¡¨ç°å¾ˆå·®. åŸå› æ˜¯å­˜åœ¨domain shiftï¼ŒCLIPåŸºäºimage-text pairçš„æ•°æ®é›†è®­ç»ƒï¼Œå»ºç«‹çš„æ˜¯å›¾ç‰‡æ•´ä½“å’Œæ–‡æœ¬æè¿°çš„è”ç³»ï¼Œä¸æ–‡æœ¬æ²¡æœ‰åšå±€éƒ¨ã€ç»†ç²’åº¦çš„å¯¹é½ . ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºregionCLIP ï¼Œå°†å›¾ç‰‡å±€éƒ¨ä¸å¯¹åº”æè¿°å¯¹é½ï¼Œåœ¨ç‰¹å¾ç©ºé—´å®ç°region-text å¯¹é½. å®éªŒè¯æ˜ï¼Œåœ¨ä¸‹æ¸¸æ£€æµ‹ä»»åŠ¡ä¸Šï¼Œä¸CLIPç›¸æ¯”æ€§èƒ½æ˜æ˜¾æå‡.  <br><br>ğŸ§Ÿâ€â™‚ï¸: pretrainæœŸé—´æ˜¾å¼å¯¹é½å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬æ ‡è®°ï¼Œæœ‰ä¸ªç‚¹ï¼Œæ— éœ€äººå·¥æ ‡æ³¨regionåŒºåŸŸcaptionï¼Œä½¿ç”¨çš„æ˜¯ä¼ªæ ‡ç­¾ï¼Œå…·ä½“åšæ³•ï¼Œå»ºç«‹ä¸€ä¸ªobjectç±»åˆ«å€™é€‰æ± ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„CLIPï¼Œå¯ä»¥å°†regionï¼ˆRPNæå–ï¼‰ä¸object matchèµ·æ¥ï¼Œç„¶ååˆ©ç”¨å›ºå®šçš„promptæ¨¡ç‰ˆç”Ÿæˆcaptionï¼Œæ¯”å¦‚regionæ˜¯ä¸€åªç‹—ï¼Œæè¿°å°±æ˜¯ a photo of dog.  è¿™é‡Œå¦‚æœå€ŸåŠ©LLaVaç­‰æ¨¡å‹ç”Ÿæˆcaptionï¼Œæ•ˆæœä¼šä¸ä¼šæ›´å¥½ï¼Ÿå½“ç„¶pretrainæˆæœ¬ä¹Ÿä¼šæ›´é«˜ï¼Œéœ€è¦balance. | <img src="./images/regionCLIP.png" width="640px"/> | [[Github](https://github.com/microsoft/RegionCLIP)] <br> [[Paper](https://arxiv.org/pdf/2112.09106)] |
| [![Star](https://img.shields.io/github/stars/allenai/reclip.svg?style=social&label=Star)](https://github.com/allenai/reclip) <br> **ReCLIP: A Strong Zero-shot Baseline for Referring Expression Comprehension** <br>| å€ŸåŠ©CLIPçš„zero-shotèƒ½åŠ›è§£å†³RECé—®é¢˜ï¼Œæå‡ºReCLIP.  ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªå…³é”®éƒ¨åˆ†: 1. Isolated Proposal Scoring (IPS) , é€šè¿‡crop proposals å’Œ æ¨¡ç³Šé™¤å½“å‰proposalä¸¤ç§æ–¹å¼ï¼Œæå–å±€éƒ¨åŒºåŸŸï¼Œé€å…¥CLIPç®—ä¸€ä¸ªscoreï¼Œä¸¤ç§æ–¹å¼åˆ†æ•°ç›¸åŠ ; 2.  Spatial Relation Resolver: å› ä¸ºCLIPå¤„ç†ç©ºé—´å…³ç³»èƒ½åŠ›æ¯”è¾ƒå¼±ï¼Œä½œè€…åˆ©ç”¨NLPçš„ä¸€äº›åŸºæœ¬æ–¹æ³•ï¼Œå€ŸåŠ©spaCyï¼Œé€šè¿‡Semantic Treesç­‰è§„åˆ™ï¼Œè®¡ç®—proposalsçš„æœ€ç»ˆå¾—åˆ†.  æ³¨æ„çš„æ˜¯ï¼Œä¸Šè¿°ä¸¤ä¸ªæ¨¡å—çš„è¾“å…¥æè¿°éåŸå§‹æè¿°ï¼Œç»è¿‡äº†æ‹†åˆ†ï¼Œå¦‚noun chunk .|<img src="./images/ReCLIP.png"  width="640px"/>| [[Github](https://github.com/allenai/reclip)] <br> [[Paper](https://arxiv.org/pdf/2204.05991)] |
|2023|
| [![Star](https://img.shields.io/github/stars/songrise/CLIP-Count.svg?style=social&label=Star)](https://github.com/songrise/CLIP-Count) <br> **CLIP-Count: Towards Text-Guided Zero-Shot Object Counting** <br>|æ–‡ç« æå‡ºä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶--CLIP-Countï¼Œå€ŸåŠ©CLIPçš„å¤šæ¨¡æ€å’Œzero-shotçš„èƒ½åŠ›æ¥é¢„æµ‹å¯†åº¦å›¾ï¼Œä½¿ç”¨æ–‡æœ¬å¼•å¯¼é›¶æ ·æœ¬æ•°é‡è®¡ç®—.  ä¸ºäº†å°†CLIPçš„image-levelèƒ½åŠ›è¿ç§»åˆ°ä¸‹æ¸¸å…³æ³¨å±€éƒ¨çš„å¯†é›†ä»»åŠ¡ä¸Šï¼Œæå‡ºpatch-text contrastive lossï¼Œå¯¹é½textå’Œpatchçš„ç‰¹å¾ç©ºé—´; åŒæ—¶ï¼Œ ä¸ºäº†å°†textual informationå¼•å…¥åˆ°å¯†dense image featuresä»¥è¿›è¡Œæ–‡æœ¬å¼•å¯¼è®¡æ•°ï¼Œä½œè€…è®¾è®¡äº†a hierarchical patch-text interactionæ¨¡å—ï¼Œå®ç°ä¸¤ä¸ªæ¨¡æ€çš„deep fusion.  å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•äººåœ¨ç¾¤è®¡æ•°æ•°æ®é›†è¡¨ç°å‡ºäº†sotaçš„å‡†ç¡®æ€§å’Œæ™®éæ€§.  |<img src="./images/CLIP-Count.png"  width="640px"/>| [[Github](https://github.com/songrise/CLIP-Count)] <br> [[Paper](https://arxiv.org/pdf/2305.07304)] |
| [![Star](https://img.shields.io/github/stars/wenwenyu/TCM.svg?style=social&label=Star)](https://github.com/wenwenyu/TCM) <br> **Turning a CLIP Model into a Scene Text Detector** <br>|å°†CLIPåº”ç”¨åˆ°æ–‡æœ¬æ£€æµ‹ä»»åŠ¡ä¸Š , ä½œè€…æå‡ºåä¸ºTCMçš„æ–‡æœ¬æ£€æµ‹æ¡†æ¶ï¼Œå¯ä»¥è½»æ¾æ’å…¥ç°æœ‰çš„æ£€æµ‹å™¨. è¯¥æ–¹æ³•ç±»ä¼¼DBNetï¼Œæ¨¡å‹è¾“å‡ºä¸€ä¸ªHeatMapæ¥å›å½’å‡ºbox.  ä¸»è¦æ”¹è¿›ç‚¹,1. promptéƒ¨åˆ†ï¼ŒPredefined  Promptï¼ˆâ€œTextâ€ï¼‰ +  Learnable Promptï¼ˆCoOpæ–¹æ³• ï¼‰ + conditional cue ï¼ˆä¸¤å±‚feed-forwardç½‘ç»œç»„æˆçš„language prompt generatorï¼Œè¾“å…¥ä¸ºclip ç¼–ç çš„å›¾åƒembeddingï¼‰. å…³äºä¸ºä»€ä¹ˆéœ€è¦å¢åŠ conditional cue ï¼Œä½œè€…è®¤ä¸ºè™½ç„¶Predefined  Promptå’ŒLearnable Promptæœ‰æ•ˆæœï¼Œä½†å¦‚æœä½¿ç”¨åœºæ™¯çš„æ•°æ®åˆ†å¸ƒä¸ä¸€è‡´ï¼Œæ¨¡å‹å¯èƒ½ä¼šå—åˆ°æœ‰é™çš„å°æ ·æœ¬æˆ–å¼€æ”¾å¼åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›çš„å½±å“.  2. visual prompt : æ–‡æœ¬-å›¾åƒä¸¤ä¸ªæ¨¡æ€ä¿¡æ¯äº¤äº’ï¼Œä»CLIPçš„å›¾åƒç¼–ç å™¨ä¸­æ¢å¤å±€éƒ¨æ€§ç‰¹å¾ï¼Œæ•è·ç»†ç²’åº¦ä¿¡æ¯ä»¥å“åº”ç²—æ–‡æœ¬åŒºåŸŸ; 3.. è®¾è®¡instance-language matchingæ–¹æ³•æ¥å¯¹é½å›¾åƒåµŒå…¥å’Œæ–‡æœ¬åµŒå…¥ï¼Œé¼“åŠ±å›¾åƒç¼–ç å™¨ä»è·¨æ¨¡å¼è§†è§‰è¯­è¨€å…ˆéªŒä¸­æ˜¾å¼ç»†åŒ–æ–‡æœ¬åŒºåŸŸ. |<img src="./images/TCM.png"  width="640px"/>| [[Github](https://github.com/wenwenyu/TCM)] <br> [[Paper](https://arxiv.org/pdf/2302.14338)] |
| [![Star](https://img.shields.io/github/stars/tgxs002/CORA.svg?style=social&label=Star)](https://github.com/tgxs002/CORA) <br> **CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching** <br>| å€ŸåŠ©VLMè§£å†³OVDé—®é¢˜ï¼Œä½œè€…è®¤ä¸ºå­˜åœ¨ä¸¤ä¸ªéš¾ç‚¹éœ€è¦è§£å†³ï¼Œ1.VLä½¿ç”¨æ•´å›¾è®­ç»ƒï¼Œç”¨åˆ°OVDä»»åŠ¡å­˜åœ¨åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼›2. å¦‚ä½•å¯¹æ²¡è§è¿‡ç±»åˆ«çš„å®šä½. æ–‡ç« é’ˆå¯¹ä»¥ä¸Šé—®é¢˜ï¼Œæå‡ºä¸€ä¸ª DETR é£æ ¼çš„æ¡†æ¶ï¼Œåä¸ºCORA.  ä½¿ç”¨Region promptingæ¥å‡è½»æ•´ä½“åˆ°å±€éƒ¨çš„åˆ†å¸ƒå·®è·ï¼›Anchor Pre-matching é€šè¿‡ç±»æ„ŸçŸ¥åŒ¹é…æœºåˆ¶å­¦ä¹ å¯æ³›åŒ–çš„å¯¹è±¡å®šä½.  å…·ä½“è€Œè¨€ï¼ŒRegion promptingåµŒåœ¨Region Classifieræ¨¡å—ï¼Œè¯¥æ¨¡å—ä¸»è¦è¿‡ç¨‹å¦‚ä¸‹ï¼šå›¾ç‰‡ç»è¿‡CLIP çš„image encoder ï¼ˆResNetç»“æ„ï¼‰çš„1-3å±‚æå–ç‰¹å¾ï¼Œç»è¿‡RoIAlignè½¬ä¸ºåŒºåŸŸç‰¹å¾ï¼Œå†ç»è¿‡CLIP çš„image encoderæœ€åä¸€å±‚ï¼Œä¹‹åä¸å¯å­¦ä¹ çš„Region Promptsé€å…ƒç´ åŠ æ³•å¾—åˆ°Region Embedding. Anchor Pre-Matchingä½¿ç”¨CLIPåˆ†ç±»æ¨¡å‹ä½œä¸ºæŒ‡æ ‡ï¼Œä¸ºæ¯ä¸€ä¸ªgroundtruthåˆ†é…ç›¸åŒæ ‡ç­¾çš„anchor boxé›†åˆ, ç›®çš„æ˜¯ä¸ºäº†å®ç°ä¸€ä¸ªèƒ½å®šä½æ½œåœ¨ç‰©ä½“çš„ç½‘ç»œ. |<img src="./images/CORA.png" width="640px"/>| [[Github](https://github.com/tgxs002/CORA)] <br> [[Paper](https://arxiv.org/pdf/2303.13076)] |
| [![Star](https://img.shields.io/github/stars/zqhang/AnomalyCLIP.svg?style=social&label=Star)](https://github.com/zqhang/AnomalyCLIP) <br> **ANOMALYCLIP: OBJECT-AGNOSTIC PROMPT LEARNING FOR ZERO-SHOT ANOMALY DETECTION** <br>|å€ŸåŠ©VLMè§£å†³é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡(ZSAD)ï¼Œä½†VLM æ›´å¤šåœ°å…³æ³¨äºå‰æ™¯ç›®æ ‡ï¼Œä¸å…³å¿ƒå›¾åƒä¸­çš„å¼‚å¸¸/æ­£å¸¸. æœ¬æ–‡æå‡ºAnomalyCLIP,  æå‡CLIPåœ¨ZSADä»»åŠ¡ä¸Šçš„è¡¨ç°.  ä½œè€…è®¤ä¸ºï¼Œå­¦ä¹ ä¸å¯¹è±¡æ— å…³çš„æ–‡æœ¬æç¤ºï¼Œæ— è®ºå…¶å‰æ™¯å¯¹è±¡å¦‚ä½•ï¼Œéƒ½å¯ä»¥æ•è·å›¾åƒä¸­çš„æ­£å¸¸å’Œå¼‚å¸¸ç‰¹å¾.  AnomalyCLIPè®¾è®¡äº†object-agnosticçš„å¯å­¦ä¹ çš„promptæ¨¡ç‰ˆï¼ˆ2ä¸ªé€šç”¨çš„ä¸å¯¹è±¡æ— å…³çš„æ–‡æœ¬æç¤ºæ¨¡æ¿ï¼Œ[object] & [damaged][object]ï¼‰, æ¥å­¦ä¹ æ­£å¸¸å’Œå¼‚å¸¸çš„æƒ…å†µ. ä½¿ç”¨glocal context optimizationï¼Œå°†globalå’Œfine-grainedçš„å¼‚å¸¸è¯­ä¹‰çº³å…¥ä¸å¯¹è±¡æ— å…³çš„æ–‡æœ¬æç¤ºå­¦ä¹ ä¸­.  æœ€ååˆ©ç”¨textual prompt tuningå’ŒDPAMåœ¨CLIPçš„textå’Œlocal visuals ç©ºé—´è¿›è¡Œpromptå­¦ä¹ .  |<img src="./images/CORA.png"  width="640px"/>| [[Github](https://github.com/zqhang/AnomalyCLIP)] <br> [[Paper](https://arxiv.org/pdf/2310.18961)] |


## Segmentation


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/raoyongming/DenseCLIP.svg?style=social&label=Star)](https://github.com/raoyongming/DenseCLIP) <br> **DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting** <br>| æ–‡ç« æå‡ºä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå°†clipçš„é¢„è®­ç»ƒçŸ¥è¯†è¿ç§»åˆ°ä¸‹æ¸¸åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ç­‰å¯†é›†ä»»åŠ¡. ä½œè€…å°†CLIP ä¸­çš„å›¾åƒ-æ–‡æœ¬åŒ¹é…é—®é¢˜è½¬æ¢ä¸ºåƒç´ æ–‡æœ¬åŒ¹é…é—®é¢˜ï¼Œå¹¶ä½¿ç”¨åƒç´ -æ–‡æœ¬åŒ¹é…é—®é¢˜ï¼Œä½¿ç”¨åƒç´ -æ–‡æœ¬åŒ¹é…å¾—åˆ†(pixel-text score maps)æ¥æŒ‡å¯¼å¯†é›†é¢„æµ‹æ¨¡å‹çš„å­¦ä¹ .  é€šè¿‡è¿›ä¸€æ­¥ä½¿ç”¨å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æç¤ºè¯­è¨€æ¨¡å‹ï¼Œä¿ƒè¿›æ¨¡å‹æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒçš„çŸ¥è¯†. |<img src="./images/DenseCLIP.png"  width="640px"/>| [[Github](https://github.com/raoyongming/DenseCLIP)] <br> [[Paper](https://arxiv.org/pdf/2112.01518)] |


## Captioning


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2021|
| [![Star](https://img.shields.io/github/stars/rmokady/CLIP_prefix_caption.svg?style=social&label=Star)](https://github.com/rmokady/CLIP_prefix_caption) <br> **ClipCap: CLIP Prefix for Image Captioning** <br>| ä½œè€…æå‡ºäº†CLIPCapæ¨¡å‹æ¥ç”Ÿæˆimage captions.å…·ä½“è€Œè¨€ï¼Œå€ŸåŠ©CLIPæå–å›¾åƒembeaddingï¼Œè®­ç»ƒä¸€ä¸ªmapping networkï¼Œä¸ºæ¯ä¸€ä¸ªcaptionç”Ÿæˆå‰ç¼€.ç›´æ¥å’Œcaption embedding åšç»“åˆï¼ˆconcatenationï¼‰ï¼Œå½¢æˆæ–°çš„embeddingï¼Œé€å…¥GPT-2ç”Ÿæˆcaptions. |<img src="./images/ClipCap.png"  width="640px"/>| [[Github](https://github.com/rmokady/CLIP_prefix_caption)] <br> [[Paper](https://arxiv.org/pdf/2111.09734)] |
|2022|
| [![Star](https://img.shields.io/github/stars/DavidHuji/CapDec.svg?style=social&label=Star)](https://github.com/DavidHuji/CapDec) <br> **CapDec: Text-Only Training for Image Captioning using Noise-Injected CLIP** <br>| æ–‡ç« è®¤ä¸ºï¼ŒClipæ¨¡å‹çš„è®­ç»ƒï¼Œå°±æ˜¯å°†æŠ½å–çš„æ–‡æœ¬å’Œå›¾ç‰‡ç‰¹å¾å°½å¯èƒ½ç›¸ä¼¼. åŸºäºè¿™ä¸ªè§‚å¯Ÿï¼Œåªéœ€è¦è®¾è®¡ä¸€ä¸ªdecoderï¼Œä»…åˆ©ç”¨æ–‡æœ¬æ•°æ®å­¦ä¹ å¦‚ä½•å°†æ–‡æœ¬ç‰¹å¾â€œç¿»è¯‘â€åˆ°æ–‡æœ¬ï¼Œå³å¯å®ç°å›¾ç‰‡captioning.  <br><br>ğŸ§Ÿâ€â™‚ï¸:è„‘æ´å¾ˆå¤§ï¼Œåˆå¾ˆåˆç†ï¼Œå–œæ¬¢è¿™ç¯‡æ–‡ç« ~ğŸ‘|<img src="./images/CapDec.png"  width="640px"/>| [[Github](https://github.com/DavidHuji/CapDec)] <br> [[Paper](https://arxiv.org/pdf/2211.00575)] |
|2023|
| [![Star](https://img.shields.io/github/stars/dhg-wei/DeCap.svg?style=social&label=Star)](https://github.com/dhg-wei/DeCap) <br> **DECAP: DECODING CLIP LATENTS FOR ZERO-SHOT CAPTIONING VIA TEXT-ONLY TRAINING** <br>|æ–‡ç« æå‡ºä¸€ä¸ªç®€å•çš„æ¡†æ¶æ¥å®ç°Zero-shot Captioning. clipçš„ text encoderä½œä¸ºè¾“å…¥ï¼Œä½¿ç”¨text-onlyæ•°æ®è®­ç»ƒä¸€ä¸ªtext decoderã€‚åŒæ—¶ï¼Œä¸ºäº†è§£å†³å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ä¸­çš„modality gapé—®é¢˜ï¼Œä½œè€…å°† image embedding é€å…¥ text decoder ä¸­è§£ç ï¼Œå®ç° Zero-shot Captioning.  |<img src="./images/DeCap.png"  width="640px"/>| [[Github](https://github.com/dhg-wei/DeCap)] <br> [[Paper](https://openreview.net/pdf?id=Lt8bMlhiwx2)] |
| [![Star](https://img.shields.io/github/stars/j-min/CLIP-Caption-Reward.svg?style=social&label=Star)](https://github.com/j-min/CLIP-Caption-Reward) <br> **Fine-grained Image Captioning with CLIP Reward** <br>| å¼€æºå›¾åƒæè¿°ç”Ÿæˆçš„æ•°æ®é›†ä¸­ï¼Œæ–‡æœ¬æè¿°é€šå¸¸æè¿°æœ€æ˜¾ç€çš„å¸¸è§å¯¹è±¡.  å› æ­¤ä½¿ç”¨è¿™äº›æ•°æ®è®­ç»ƒçš„å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹ï¼Œå¾€å¾€ä¼šå¿½ç•¥å›¾åƒä¹‹é—´çš„çš„ç‰¹å®šå’Œè¯¦ç»†ç»†èŠ‚.  æœ¬æ–‡ä¸ºäº†ç”Ÿæˆæ›´å…·æè¿°æ€§å’Œç‹¬ç‰¹çš„æè¿°ï¼Œä½¿ç”¨CLIPçš„å›¾åƒæ–‡æœ¬ç›¸ä¼¼åº¦ä½œä¸ºæè¿°ç”Ÿæˆæ¨¡å‹çš„rewardï¼ŒåŒæ—¶ï¼Œæ–‡ç« è¿˜æå‡ºä¸€ç§ CLIP æ–‡æœ¬ç¼–ç å™¨finetuneçš„ç­–ç•¥ï¼Œä»¥æ”¹è¿›æ–‡æœ¬ç¼–ç å™¨çš„è¯­æ³•èƒ½åŠ›. |<img src="./images/CLIP-Caption-Reward.png"  width="640px"/>| [[Github](https://github.com/j-min/CLIP-Caption-Reward)] <br> [[Paper](https://arxiv.org/pdf/2205.13115)] |


## Generation


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/hila-chefer/TargetCLIP.svg?style=social&label=Star)](https://github.com/hila-chefer/TargetCLIP) <br> **Image-based CLIP-Guided Essence Transfer** <br>| StyleGANä¸CLIPç»“åˆå®ç°äººè„¸çš„é£æ ¼è¿ç§». <br><br>ğŸ§Ÿâ€â™‚ï¸: ç•¥è¯»äº†ä¸€ä¸‹ï¼Œä¹‹å‰åšè¿‡ä¸€æ®µæ—¶é—´çš„Ganï¼Œå¯¹StyleGanè¿˜æ˜¯æ¯”è¾ƒäº†è§£çš„.  CLIPä¹‹å‰ï¼ŒåŸºäºStyleGanåšä¸€äº›äººè„¸å±æ€§ç¼–è¾‘ï¼Œæ–¹æ³•åŸºæœ¬å¤§åŒå°å¼‚ï¼Œæ•ˆç”¨encoderæŠŠå›¾ç‰‡æ˜ å°„åˆ°latentç©ºé—´ï¼Œç„¶ååœ¨è¿™ä¸ªlatentç©ºé—´å¯¹è¿™ä¸ªå‘é‡åšä¸€äº›æ‰‹è„šï¼Œæ¯”å¦‚å¯ä»¥æŸä¸ªæ–¹å‘åç§»ã€é¢å¤–åˆ©ç”¨ä¸€äº›åˆ†ç±»ã€æˆ–è€…åšfusionç­‰.  æœ¬æ–‡ä¾æ—§æ˜¯latentç©ºé—´å»åŠ ä¸€ä¸ªshift vectorï¼Œæ–‡ç« ç§°ä½œessence vectorï¼Œç„¶ååˆ©ç”¨CLIP çš„image encoderå»æ–½åŠ ç›‘ç£. æ–‡ç« ç»™çš„ç”Ÿæˆcaseçœ‹ç€æ•ˆæœè¿˜å¯ä»¥ï¼Œå…·ä½“è¿˜éœ€è¦æµ‹è¯•. | <img src="./images/TargetCLIP.png"  width="640px"/>  | [[Github](https://github.com/hila-chefer/TargetCLIP)] <br> [[Paper](https://arxiv.org/pdf/2110.12427)] |
| [![Star](https://img.shields.io/github/stars/yael-vinker/CLIPasso.svg?style=social&label=Star)](https://github.com/yael-vinker/CLIPasso) <br> **CLIPasso: Semantically-Aware Object Sketching** <br>| SIGGRAPH 2022 (Best Paper Award) ä¸€ç§å°†å›¾åƒä¸­çš„ç‰©ä½“è½¬æ¢ä¸ºè‰å›¾çš„æ–¹æ³•ï¼Œå…è®¸ä¸åŒå±‚æ¬¡çš„æŠ½è±¡.  <br><br>ğŸ§Ÿâ€â™‚ï¸å…³é”®è¯: æ˜¾ç€æ€§å¼•å¯¼çš„åˆå§‹åŒ–(a saliency-guided initialization)ã€åŸºäºå±€éƒ¨æ³¨æ„åŠ›å›¾ï¼ˆbased on the local attention mapsï¼‰ã€BÂ´ezier curvesï¼ˆè´å¡å°”æ›²çº¿æ˜¯é€šè¿‡ä¸€ç»„æ§åˆ¶ç‚¹ï¼ˆP0...Pnï¼‰æ¥å®šä¹‰çš„ï¼ŒP0å’ŒPnæ˜¯æ›²çº¿çš„èµ·ç‚¹å’Œç»ˆç‚¹,ä¸­é—´æ§åˆ¶ç‚¹é€šå¸¸ä¸åœ¨æ›²çº¿ä¸Šï¼‰. é¦–å…ˆé€šè¿‡æ˜¾è‘—å›¾åˆå§‹åŒ–è‰å›¾çš„åˆå§‹ç‚¹(é€šè¿‡çƒ­åŠ›å›¾)ï¼Œç„¶åå»è¿­ä»£æ›´æ–°BÂ´ezier curves å‚æ•°ï¼ˆé‚£ä¸€ç»„ç‚¹ï¼‰ï¼Œé€šè¿‡ä¸€ä¸ª[rasterizer](https://dl.acm.org/doi/pdf/10.1145/3414685.3417871)ç»˜åˆ¶å‡ºè‰å›¾ï¼Œå€ŸåŠ©CLIPè®¡ç®—lossï¼Œæ›´æ–°é‚£ä¸€ç»„ç‚¹ ï¼Œç›´åˆ°æ”¶æ•›| <img src="./images/CLIPasso.png"  width="640px"/>  | [[Github](https://github.com/yael-vinker/CLIPasso)] <br> [[Paper](https://arxiv.org/pdf/2202.05822)] |
|2024|
| **CLIP4Sketch: Enhancing Sketch to Mugshot Matching through Dataset Augmentation using Diffusion Models** <br>| æ–‡ç« æå‡ºä¸€ç§ç”Ÿæˆä¸åŒç±»å‹è‰å›¾çš„æ–¹æ³•ï¼Œåä¸ºCLIP4Sketch.  è¯¥æ–¹æ³•åˆ©ç”¨DDPMæ¥ç”Ÿæˆè‰å›¾ï¼Œä½¿ç”¨CLIP å’Œ Adaface åˆ†åˆ«æå–è¾“å…¥å‚è€ƒå›¾ç‰‡çš„embeddingï¼ŒäºŒè€…fusionåä¸è‰å›¾é£æ ¼çš„æ–‡æœ¬æè¿°embeddingå†æ¬¡fusionï¼Œä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ï¼Œç²¾ç¡®æ§åˆ¶äººç‰©èº«ä»½å’Œè‰å›¾çš„é£æ ¼. ä½œè€…åˆ©ç”¨CLIP4Sketchç”Ÿæˆæ•°æ®ï¼Œå¹¶è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ï¼ŒéªŒè¯äº†è¯¥æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§.  | <img src="./images/CLIP4Sketch.png"  width="640px"/>  | [[Paper](https://arxiv.org/pdf/2408.01233)] |


## Video


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/OpenGVLab/efficient-video-recognition.svg?style=social&label=Star)](https://github.com/OpenGVLab/efficient-video-recognition) <br> **Frozen CLIP Models are Efficient Video Learners** <br>| ä½œè€…è®¤ä¸ºé’ˆå¯¹è§†é¢‘è¯†åˆ«ä»»åŠ¡ï¼Œç«¯åˆ°ç«¯çš„è®­ç»ƒã€å¾®è°ƒæ¨¡å‹çš„æ•ˆç‡ä¸é«˜ï¼Œæ­¤å¤–ï¼Œå¦‚æœæƒ³è¦å€ŸåŠ©CLIPçš„å¼ºå¤§zero-shotèƒ½åŠ›ï¼Œä½†å´è¦å¾®è°ƒæ¨¡å‹ï¼Œä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜.  é¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„ç«¯åˆ°ç«¯å¾®è°ƒå¹¶ä¸æ€»æ˜¯ç†æƒ³çš„é€‰æ‹©ï¼Œè¿™éœ€è¦æ›´æœ‰æ•ˆçš„å­¦ä¹ ç­–ç•¥æ¥å°†çŸ¥è¯†ä»å›¾åƒè½¬ç§»åˆ°è§†é¢‘ï¼Œæœ¬æ–‡æå‡ºEfficient Video Learning (EVL) ï¼Œç›´æ¥å°†CLIPæ¨¡å‹å‚æ•°freezeï¼Œè®­ç»ƒä¸€ä¸ªé«˜è´¨é‡è§†é¢‘è¯†åˆ«æ¨¡å‹.  ä½¿ç”¨è½»é‡çº§çš„ Transformer è§£ç å™¨ä½œä¸ºåŸºç¡€ç½‘ç»œï¼Œä½¿ç”¨freezed CLIPæå–è§†é¢‘å¸§çš„ç©ºé—´ç‰¹å¾ä½œä¸ºKVé€å…¥decoderï¼ŒåŒæ—¶ä½œè€…è®¾è®¡äº†local temporal æ¨¡å—æ¥å­¦ä¹ å¸§é—´çš„æ—¶åŸŸå…³ç³».|<img src="./images/efficient-video-recognition.png"  width= "640px"/>| [[Github](https://github.com/OpenGVLab/efficient-video-recognition)] <br> [[Paper](https://arxiv.org/pdf/2208.03550)] |
|2023|
| [![Star](https://img.shields.io/github/stars/alibaba-mmai-research/CLIP-FSAR.svg?style=social&label=Star)](https://github.com/alibaba-mmai-research/CLIP-FSAR) <br> **CLIP-guided Prototype Modulating for Few-shot Action Recognition** <br>| æ–‡ç« æå‡ºåä¸ºCLIP-FSARçš„æ¡†æ¶ï¼Œå°†CLIPå¼•å…¥å°æ ·æœ¬åŠ¨ä½œè¯†åˆ«ä»»åŠ¡ï¼ˆfew-shot action recognitionï¼‰. CLIP-FSARå……åˆ†åˆ©ç”¨äº† CLIP æ¨¡å‹çš„å¤šæ¨¡æ€çŸ¥è¯†ï¼Œå¹¶è®¾è®¡äº†è§†é¢‘-æ–‡æœ¬å¯¹æ¯”lossæ¥æ¨¡æ‹ŸåŸå§‹CLIPçš„è®­ç»ƒæ¨¡å¼ï¼Œä»¥åŠprototype modulationæ¥ç”Ÿæˆå¯é çš„åŸå‹.|<img src="./images/CLIP-FSAR.png"  width= "640px"/>| [[Github](https://github.com/alibaba-mmai-research/CLIP-FSAR)] <br> [[Paper](https://arxiv.org/pdf/2303.02982)] |
| [![Star](https://img.shields.io/github/stars/muzairkhattak/ViFi-CLIP.svg?style=social&label=Star)](https://github.com/muzairkhattak/ViFi-CLIP) <br> **Fine-tuned CLIP Models are Efficient Video Learners** <br>| æ–‡ç« æ¢ç´¢å¦‚ä½•å°†CLIPçš„å¼ºå¤§èƒ½åŠ›ä»å›¾ç‰‡åŸŸè¿ç§»åˆ°è§†é¢‘åŸŸ.  æå‡ºVideo Fine-tuned CLIP (ViFi-CLIP)ï¼Œè¯æ˜å…¶å¯ä»¥å¼¥åˆä»å›¾åƒåˆ°è§†é¢‘çš„domain gap . é’ˆå¯¹æ•°æ®é‡ä¸è¶³ä»¥å…¨é‡å¾®è°ƒçš„æƒ…å†µï¼Œæå‡ºäº†bridge and prompt çš„æ–¹æ³•æ¥å‡å°ä¸Šè¿°gap.  <br><br>ğŸ§Ÿâ€â™‚ï¸: ã€ŠFrozen CLIP Models are Efficient Video Learnersã€‹ vs  ã€ŠFine-tuned CLIP Models are Efficient Video Learnersã€‹ï¼Œæ‰“ä¸€æ¶å§ï½|<img src="./images/ViFi-CLIP.png"  width= "640px"/>| [[Github](https://github.com/muzairkhattak/ViFi-CLIP)] <br> [[Paper](https://arxiv.org/pdf/2212.03640)] |

|2024|
| [![Star](https://img.shields.io/github/stars/Visual-AI/FROSTER.svg?style=social&label=Star)](https://github.com/Visual-AI/FROSTER) <br> **FROSTER: FROZEN CLIP IS A STRONG TEACHER FOR OPEN-VOCABULARY ACTION RECOGNITION** <br>|æœ¬æ–‡çš„ç ”ç©¶è¯¾é¢˜æ˜¯å¼€é›†åŠ¨ä½œè¯†åˆ«ï¼ˆopen-vocabulary action recognitionï¼‰ï¼Œå…·ä½“æ¥è¯´å°±æ˜¯æµ‹è¯•é›†ä¸­çš„è§†é¢‘åŠ¨ä½œç±»åˆ«ä¸è®­ç»ƒé›†åŠ¨ä½œç±»åˆ«åŸºæœ¬æ²¡æœ‰é‡å æˆ–é‡å ç¨‹åº¦å¾ˆå°ï¼Œå› æ­¤è¿™éœ€è¦æ¨¡å‹å…·å¤‡è¾ƒé«˜çš„æ³›åŒ–æ€§èƒ½. ç›®å‰è§†é¢‘é¢†åŸŸä¸»æµçš„åšæ³•æ˜¯åŸºäºå›¾åƒ-æ–‡æœ¬å¯¹é¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆä¸»è¦æ˜¯CLIPï¼‰å…ˆåœ¨è§†é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œfine-tuningï¼Œç„¶åå†è¿›è¡Œæµ‹è¯•é›†çš„éªŒè¯. é€šè¿‡å®éªŒæ¢ç´¢ï¼Œæˆ‘ä»¬å‘ç°ï¼šå°½ç®¡fine-tuningå¯ä»¥è®©CLIPå…·å¤‡ä¸é”™çš„è§†é¢‘ç‰¹å¾æå–çš„èƒ½åŠ›ï¼Œä½†è¿™ä¹Ÿä¼šè®©å®ƒå¤±å»å¤§è§„æ¨¡é¢„è®­ç»ƒæ‰€å¾—åˆ°çš„æ³›åŒ–æ€§èƒ½.  å…·ä½“çš„è¡¨ç°å°±æ˜¯ï¼Œé‚£äº›åœ¨é—­é›†ï¼ˆclosed-setï¼‰åœºæ™¯ä¸‹ä¼˜ç§€çš„è§†é¢‘åˆ†ç±»å™¨ä»¬ï¼Œä¸€åˆ°äº†å¼€é›†åœºæ™¯ä¸‹å®éªŒæ€§èƒ½ä¾¿å¤§å¤§ç¼©æ°´ï¼Œç”šè‡³ä¸å¦‚åŸå…ˆçš„é¢„è®­ç»ƒCLIPæ¨¡å‹äº†. å› æ­¤å¦‚ä½•è®©è§†é¢‘æ¨¡å‹åœ¨fine-tuningçš„åŒæ—¶è¿˜èƒ½ä¿æŒä½é¢„è®­ç»ƒçš„çŸ¥è¯†ï¼Œæˆä¸ºäº†æœ¬æ–‡çš„ç ”ç©¶é‡ç‚¹.  ä½œè€…è®¤ä¸ºä¸€ä¸ªåŸºäºCLIPçš„å¼€é›†åŠ¨ä½œè¯†åˆ«æ¨¡å‹åº”è¯¥å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š1.ç”±äºCLIPé¢„è®­ç»ƒæ˜¯æ²¡æœ‰ä½¿ç”¨è§†é¢‘æ•°æ®é›†çš„ï¼Œå› æ­¤æ¨¡å‹éœ€è¦å­¦ä¹ è§†é¢‘åŸŸçš„ç›¸å…³çŸ¥è¯†ï¼ˆvideo-specificï¼‰ï¼Œç”¨äºå¼¥è¡¥CLIPåœ¨æ—¶åŸŸå»ºæ¨¡æ–¹é¢çš„ä¸è¶³; 2æ¨¡å‹éœ€è¦èƒ½ä¿æŒä½é¢„è®­ç»ƒCLIPçš„èƒ½åŠ›ï¼Œè¿™å¯¹äºæ³›åŒ–æ€§èƒ½åŠ›çš„ä¿æŒå¾ˆé‡è¦.   ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ç»“æ„FROSTERç”¨æ¥åŒæ—¶å®ç°ä»¥ä¸Šä¸¤ä¸ªç›®æ ‡ï¼šé’ˆå¯¹ç¬¬ä¸€ç‚¹ï¼ˆæ—¶åŸŸå»ºæ¨¡ï¼‰ï¼Œç›´æ¥é‡‡ç”¨cross-entropy losså¯¹finetuneæ¨¡å‹è¿›è¡Œç›‘ç£. é’ˆå¯¹ç¬¬äºŒç‚¹ï¼ˆæ³›åŒ–æ€§ç‰¹å¾ä¿æŒï¼‰ï¼Œå°†frozen clipä½œä¸ºteacheræ¨¡å‹å¯¹fine-tuneæ¨¡å‹çš„ç‰¹å¾è¿›è¡Œè’¸é¦ï¼Œå€Ÿæ­¤å¸Œæœ›é¢„è®­ç»ƒçš„èƒ½åŠ›èƒ½å¤Ÿå¾—åˆ°å¾ˆå¥½åœ°ä¿æŒ. è’¸é¦è¿‡ç¨‹ç±»ä¼¼äºä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œç¡®ä¿fine-tuneç‰¹å¾ä¸ä¼šåç¦»frozen clipçš„ç‰¹å¾å¤ªè¿œ. å› ä¸ºæœ‰ä¸¤ä¸ªä¸åŒçš„ç›®æ ‡ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å®ƒä»¬ä¹‹é—´å¹³è¡¡ç‰¹å¾å­¦ä¹ . |<img src="./images/FROSTER.png"  width="640px"/>| [[Github](https://github.com/Visual-AI/FROSTER)] <br> [[Paper](https://hal.science/hal-04534868v1/document)] <br> [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/681708426) |
| [![Star](https://img.shields.io/github/stars/XiaoBuL/OmniCLIP.svg?style=social&label=Star)](https://github.com/XiaoBuL/OmniCLIP) <br> **OmniCLIP: Adapting CLIP for Video Recognition with Spatial-Temporal Omni-Scale Feature Learning** <br>| æ–‡ç« æå‡ºOmniCLIPï¼Œå°†CLIPç”¨äºè§†é¢‘è¯†åˆ«. åœ¨è§†é¢‘è¯†åˆ«ä»»åŠ¡ä¸Šï¼Œä½œè€…è®¤ä¸ºæ—¶é—´ä¿¡æ¯çš„èåˆåº”è¯¥æ˜¯å…¨æ–¹ä½çš„ï¼ŒåŒ…æ‹¬ç©ºé—´ã€æ—¶é—´å’Œæ—¶ç©º. åŸºäºè¿™ä¸ªè§‚ç‚¹ï¼ŒOmniCLIP é‡ç‚¹å­¦ä¹ æ¶µç›–ç©ºé—´ã€æ—¶é—´å’ŒåŠ¨æ€æ—¶ç©ºå°ºåº¦çš„ç»¼åˆç‰¹å¾ï¼Œä½œè€…å°†å…¶ç§°ä¸ºomni-scale features.  OmniCLIPä½¿ç”¨Parallel Temporal Adapter (PTA) æ¨¡å—å¢å¼º CLIP åœ¨æ—¶é—´å»ºæ¨¡æ–¹é¢çš„èƒ½åŠ›ï¼Œä»¥åŠSelf-Prompt Generator (SPG) æ¨¡å—æ¥å¤„ç†ä¸åŒç©ºé—´å°ºåº¦ä¸Šå¯¹è±¡çš„åŠ¨æ€äº¤äº’å’Œä¸è§„åˆ™è¿åŠ¨. å®éªŒè¯æ˜ï¼ŒOmniCLIP åœ¨HMDB51ç­‰æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚. | <img src="./images/C2P-CLIP.png " width="640px"/> | [[Github](https://github.com/chuangchuangtan/XiaoBuL/OmniCLIP)] <br> [[Paper](https://arxiv.org/pdf/2408.06158)] |

## Other


| Title | Abstract | Intro | Useful Links |
|:----| :---:| :----: | :---:|
|2022|
| [![Star](https://img.shields.io/github/stars/Sense-GVT/DeCLIP.svg?style=social&label=Star)](https://github.com/Sense-GVT/DeCLIP) <br> **Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision** <br>| æ–‡ç« æå‡ºCLIP-benchmarkï¼Œæ˜¯ç¬¬ä¸€ä¸ªå¯¹CLIPåŠå…¶å˜ä½“è¿›è¡Œè¯„ä¼°ã€åˆ†æå’Œæµ‹è¯•çš„åŸºå‡†. åŒæ—¶ï¼Œä½œè€…æå‡ºä¸‰ä¸ªè§‚ç‚¹ï¼Œ1.æ•°æ®è´¨é‡å¯¹æ€§èƒ½æœ‰å¾ˆå¤§å½±å“ï¼›2..æŸäº›supervisionå¯¹å·ç§¯ç½‘ç»œï¼ˆConvNetsï¼‰å’Œè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰æœ‰ä¸åŒçš„å½±å“. é€‚å½“çš„supervisionå¯ä»¥æœ‰æ•ˆåœ°æé«˜CLIPçš„æ€§èƒ½; 3.å‡å°‘æ–‡æœ¬ç¼–ç å™¨å¯ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼Œä½†å¯¹æœ€ç»ˆæ€§èƒ½å½±å“ä¸å¤§.  æ­¤å¤–ï¼Œä½œè€…å°†DeCLIPä¸FLIPç»“åˆï¼Œå¾—åˆ°ä¸€ä¸ªæ€§èƒ½è¾ƒå¥½çš„CLIPå˜ä½“: DeFILIP.|| [[Github](https://github.com/Sense-GVT/DeCLIP)] <br> [[Paper](https://arxiv.org/pdf/2203.05796)] |
| [![Star](https://img.shields.io/github/stars/IceClear/CLIP-IQA.svg?style=social&label=Star)](https://github.com/IceClear/CLIP-IQA) <br> **Exploring CLIP for Assessing the Look and Feel of Images** <br>| å€ŸåŠ©CLIPåšIQAï¼Œæ–‡ç« å®éªŒè¯æ˜ï¼ŒCLIP æ•è·äº†æœ‰æ„ä¹‰çš„priorsï¼Œå¯ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°ä¸åŒçš„æ„ŸçŸ¥è¯„ä¼° . | <img src="./images/CLIP-IQA.png" width="640px"/> | [[Github](https://github.com/IceClear/CLIP-IQA)] <br> [[Paper](https://arxiv.org/pdf/2207.12396)] |
| [![Star](https://img.shields.io/github/stars/GuyTevet/MotionCLIP.svg?style=social&label=Star)](https://github.com/GuyTevet/MotionCLIP) <br> **CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded
3D Recognition** <br>| æœ¬æ–‡æå‡ºMotionCLIPï¼Œä¸€ç§ 3D äººä½“è¿åŠ¨auto-encoderï¼Œå…·æœ‰ç‰¹å¾ç©ºé—´è§£ç¼ ç»“ã€ä¸é«˜åº¦è¯­ä¹‰çš„æ–‡æœ¬æè¿°ç‰¹å¾å¯¹é½çš„ç‰¹æ€§. MotionCLIPå°†äººä½“è¿åŠ¨æµå½¢ä¸ CLIP ç©ºé—´å¯¹é½ï¼Œéšå¼åœ°å°† CLIP æå…¶ä¸°å¯Œçš„è¯­ä¹‰çŸ¥è¯†æ³¨å…¥åˆ°æµå½¢ä¸­. æ–‡ç« åŒæ—¶è¯æ˜ï¼Œå°½ç®¡ CLIP ä»æœªè§è¿‡è¿åŠ¨åŸŸï¼Œä½†MotionCLIP æä¾›äº†å‰æ‰€æœªæœ‰çš„æ–‡æœ¬åˆ°è¿åŠ¨çš„èƒ½åŠ›ï¼Œå…è®¸åŸŸå¤–æ“ä½œã€è§£çº ç¼ çš„ç¼–è¾‘å’ŒæŠ½è±¡è¯­è¨€è§„èŒƒçš„èƒ½åŠ›. | <img src="./images/MotionCLIP.png" width="640px"/> | [[Github](https://github.com/GuyTevet/MotionCLIP)] <br> [[Paper](https://arxiv.org/pdf/2203.08063)] |
| [![Star](https://img.shields.io/github/stars/deeptibhegde/CLIP-goes-3D.svg?style=social&label=Star)](https://github.com/deeptibhegde/CLIP-goes-3D) <br> **CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded
3D Recognition** <br>| æ–‡ç« æå‡ºäº†ä¸€ä¸ªåä¸º CG3Dï¼ˆCLIP Goes 3Dï¼‰çš„æ–°é¢„è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨è‡ªç„¶è¯­è¨€ç›‘ç£æ¥è®­ç»ƒ 3D ç¼–ç å™¨ï¼ŒåŒæ—¶è¿˜æ³¨å…¥äº†CLIP zero-shotèƒ½åŠ›. | <img src="./images/CLIP-goes-3D.png" width="640px"/> | [[Github](https://github.com/deeptibhegde/CLIP-goes-3D)] <br> [[Paper](https://arxiv.org/pdf/2303.11313)] |
|2024|
| [![Star](https://img.shields.io/github/stars/MrChenFeng/CLIPCleaner_ACMMM2024.svg?style=social&label=Star)](https://github.com/MrChenFeng/CLIPCleaner_ACMMM2024) <br> **CLIPCleaner: Cleaning Noisy Labels with CLIP** <br>| æ–‡ç« é¦–æ¬¡æå‡ºåˆ©ç”¨ VLæ¨¡å‹è¿›è¡Œæ ·æœ¬é€‰æ‹©æ¥è§£å†³å™ªå£°æ ‡ç­¾å­¦ä¹  (LNL) é—®é¢˜.  ä½œè€…ä½¿ç”¨CLIPï¼Œå€ŸåŠ©å…¶å¤©ç„¶çš„å¤šæ¨¡æ€å’Œzero-shotèƒ½åŠ›ï¼Œæå‡ºä¸€ç§CLIPCleanerçš„æ–¹æ³•æ¥è¿›è¡Œæ ·æœ¬é€‰æ‹©ï¼Œå¹¶ä»ç†è®ºå’Œç»éªŒä¸Šè¯å®äº†å…¶æœ‰æ•ˆæ€§ï¼ˆå¤§é‡å…¬å¼è­¦å‘Šï¼‰ . | <img src="./images/CLIPCleaner.png" width="640px"/> | [[Github](https://github.com/MrChenFeng/CLIPCleaner_ACMMM2024)] <br> [[Paper](https://www.arxiv.org/pdf/2408.10012)] |
| [![Star](https://img.shields.io/github/stars/chuangchuangtan/C2P-CLIP-DeepfakeDetection.svg?style=social&label=Star)](https://github.com/chuangchuangtan/C2P-CLIP-DeepfakeDetection) <br> **C2P-CLIP: Injecting Category Common Prompt in CLIP to Enhance Generalization in Deepfake Detection** <br>| æ–‡ç« æ¶‰åŠAIGC æ£€æµ‹æ–¹å‘ï¼Œæå‡ºä¸€ç§èƒ½å¤Ÿè¯†åˆ«å„ç§ä¼ªé€ å›¾åƒçš„æ–¹æ³•.  é¦–å…ˆä½œè€…é€šè¿‡å®éªŒåˆ†æCLIPï¼Œè®¤ä¸ºCLIP å¯ä»¥é€šè¿‡è¯†åˆ«ç›¸ä¼¼çš„æ¦‚å¿µæ¥æ£€æµ‹æ·±åº¦ä¼ªé€ å“. åŸºäºæ­¤ï¼Œå¼•å…¥ç±»åˆ«é€šç”¨æç¤ºCLIPï¼ˆCategory Common Prompt CLIPï¼‰--C2P-CLIPï¼Œå®ƒå°†ç±»åˆ«é€šç”¨æç¤ºé›†æˆåˆ°æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼Œå°†ç±»åˆ«ç›¸å…³æ¦‚å¿µæ³¨å…¥åˆ°å›¾åƒç¼–ç å™¨ä¸­ï¼Œä»è€Œå¢å¼ºæ£€æµ‹æ€§èƒ½.  å®éªŒè¯æ˜ï¼Œæ­¤æ–¹æ¡ˆåœ¨UniversalFakeDetect å’ŒGenImage ä¸Šå–å¾—äº†SOTAçš„æ•ˆæœ.| <img src="./images/C2P-CLIP.png" width="640px"/> | [[Github](https://github.com/chuangchuangtan/C2P-CLIP-DeepfakeDetection)] <br> [[Paper](https://arxiv.org/pdf/2408.09647)] |

